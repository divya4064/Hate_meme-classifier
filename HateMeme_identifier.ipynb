{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1fe3eb",
      "metadata": {
        "id": "aa1fe3eb"
      },
      "outputs": [],
      "source": [
        "!export CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ad6968",
      "metadata": {
        "id": "b6ad6968"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
        "# import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c705e1c",
      "metadata": {
        "id": "2c705e1c"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ced09b62",
      "metadata": {
        "id": "ced09b62"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# clip_model, compose = clip.load('ViT-L/14', device = device)\n",
        "# text_model = text_model.cpu()\n",
        "# def process(idx_val,arr):\n",
        "#   if idx_val=='0':\n",
        "#     arr.append(0)\n",
        "#   else:\n",
        "#     arr.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94bac190",
      "metadata": {
        "id": "94bac190"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a344ef26",
      "metadata": {
        "id": "a344ef26"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score,precision_score\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import distutils.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1f3155",
      "metadata": {
        "id": "3b1f3155"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/DATA/atul_2221cs20/jitendra/MAMI_train_set.csv')  #Drive link where your data is saved\n",
        "data_test = pd.read_csv('/DATA/atul_2221cs20/jitendra/MAMI_test_set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6ce86d",
      "metadata": {
        "id": "0f6ce86d",
        "outputId": "4649b59e-ac70-42cb-fc24-3d071508b00e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>shaming</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>objectification</th>\n",
              "      <th>violence</th>\n",
              "      <th>Text_Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10007.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>PICTOPHLE APP *Straight white malle starts tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10008.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Chinese restaurant faces closure after 30 year...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10009.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>YOU WERE THE CHOSEN ONE! YOU WERE MEANT TO DES...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1001.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>* 84% 6:07 PM Pull start for dishwasher $19 Li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10010.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>SCIENTISTS HAVE DISCOVERED THAT THERE IS INTEL...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
              "0      1.jpg           0        0           0                0         0   \n",
              "1     10.jpg           1        0           0                0         1   \n",
              "2   1000.jpg           0        0           0                0         0   \n",
              "3  10000.jpg           0        0           0                0         0   \n",
              "4  10006.jpg           0        0           0                0         0   \n",
              "5  10007.jpg           0        0           0                0         0   \n",
              "6  10008.jpg           0        0           0                0         0   \n",
              "7  10009.jpg           0        0           0                0         0   \n",
              "8   1001.jpg           1        0           1                1         1   \n",
              "9  10010.jpg           1        0           0                1         0   \n",
              "\n",
              "                                  Text_Transcription  \n",
              "0                                      Milk Milk.zip  \n",
              "1                                      ROSES ARE RED  \n",
              "2  BREAKING NEWS: Russia releases photo of DONALD...  \n",
              "3                       MAN SEEKING WOMAN Ignad 18 O  \n",
              "4  Me explaining the deep lore of. J.R.R. Tolkein...  \n",
              "5  PICTOPHLE APP *Straight white malle starts tal...  \n",
              "6  Chinese restaurant faces closure after 30 year...  \n",
              "7  YOU WERE THE CHOSEN ONE! YOU WERE MEANT TO DES...  \n",
              "8  * 84% 6:07 PM Pull start for dishwasher $19 Li...  \n",
              "9  SCIENTISTS HAVE DISCOVERED THAT THERE IS INTEL...  "
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778bf4e7",
      "metadata": {
        "id": "778bf4e7",
        "outputId": "a572f3eb-7a89-46ff-8e7d-9decc7a85a80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>shaming</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>objectification</th>\n",
              "      <th>violence</th>\n",
              "      <th>Text_Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15001.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G HIS. UNDYING FIDELITY Steve is hot and perfe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15002.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>How limagined myself as a Teacher...... How I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15004.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>WHERE WILL YOU BE WHEN DIARRHEA STRIKE memecen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15005.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A MAN WITH DREAMS... NEEDS A WOMAN WITH VISION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>THIS IS HOW YOUR GIRLFRIEND SEES YOUR FEMALE F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15008.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>BITCHES BE LIKE... GOIN TO THE HIGHEST BIDDER ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>15009.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>MARRIAGE IS JUST TEXTING EACH OTHER MEMES FROM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>15010.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>MARRIAGE IS... ALWAYS HAVING SOMEONE TO PITY-L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>15011.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>THE MEDIA: THIS IS A PEACEFUL PROTEST ALSO THE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15012.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>PARENTING MOMSGOTINK IF YOU FEEL CRAZY, THEN Y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
              "0  15001.jpg           0        0           0                0         0   \n",
              "1  15002.jpg           0        0           0                0         0   \n",
              "2  15004.jpg           0        0           0                0         0   \n",
              "3  15005.jpg           0        0           0                0         0   \n",
              "4  15006.jpg           0        0           0                0         0   \n",
              "5  15008.jpg           1        0           0                1         1   \n",
              "6  15009.jpg           0        0           0                0         0   \n",
              "7  15010.jpg           0        0           0                0         0   \n",
              "8  15011.jpg           0        0           0                0         0   \n",
              "9  15012.jpg           0        0           0                0         0   \n",
              "\n",
              "                                  Text_Transcription  \n",
              "0  G HIS. UNDYING FIDELITY Steve is hot and perfe...  \n",
              "1  How limagined myself as a Teacher...... How I ...  \n",
              "2  WHERE WILL YOU BE WHEN DIARRHEA STRIKE memecen...  \n",
              "3     A MAN WITH DREAMS... NEEDS A WOMAN WITH VISION  \n",
              "4  THIS IS HOW YOUR GIRLFRIEND SEES YOUR FEMALE F...  \n",
              "5  BITCHES BE LIKE... GOIN TO THE HIGHEST BIDDER ...  \n",
              "6  MARRIAGE IS JUST TEXTING EACH OTHER MEMES FROM...  \n",
              "7  MARRIAGE IS... ALWAYS HAVING SOMEONE TO PITY-L...  \n",
              "8  THE MEDIA: THIS IS A PEACEFUL PROTEST ALSO THE...  \n",
              "9  PARENTING MOMSGOTINK IF YOU FEEL CRAZY, THEN Y...  "
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_test[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75133380",
      "metadata": {
        "id": "75133380",
        "outputId": "42ce965d-bb23-45bf-c918-fae91c1260e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>shaming</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>objectification</th>\n",
              "      <th>violence</th>\n",
              "      <th>Text_Transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
              "0     1.jpg           0        0           0                0         0   \n",
              "\n",
              "  Text_Transcription  \n",
              "0      Milk Milk.zip  "
            ]
          },
          "execution_count": 285,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b89beb",
      "metadata": {
        "id": "b2b89beb"
      },
      "outputs": [],
      "source": [
        "#!pip install multilingual-clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a34a6f7",
      "metadata": {
        "id": "8a34a6f7"
      },
      "outputs": [],
      "source": [
        "from multilingual_clip import pt_multilingual_clip\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0ab191",
      "metadata": {
        "id": "4d0ab191",
        "outputId": "04257c65-08f8-42c7-f2b3-196559dad93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[49406,  1139,   531,   886,  1137, 42578, 49407,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0',\n",
            "       dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# clip_model, compose = clip.load('RN50x4', device = device)\n",
        "clip_model, compose = clip.load(\"ViT-B/32\", device = device)\n",
        "text_inputs = (clip.tokenize(data.Text_Transcription.values[321],truncate=True)).to(device)\n",
        "print(text_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd851abf",
      "metadata": {
        "id": "bd851abf"
      },
      "outputs": [],
      "source": [
        "t_f=clip_model.encode_text(text_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d1aa4e",
      "metadata": {
        "id": "b9d1aa4e"
      },
      "outputs": [],
      "source": [
        "#clip_model.encode_text(torch.tensor([[1]*77]).cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001b00e5",
      "metadata": {
        "id": "001b00e5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c655d774",
      "metadata": {
        "id": "c655d774"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01d4591",
      "metadata": {
        "id": "a01d4591"
      },
      "outputs": [],
      "source": [
        "def get_data(data):  # data = pd.read_csv(dataset_path)\n",
        "    # Extracting lists from the DataFrame\n",
        "    text = list(data['Text_Transcription'])\n",
        "    img_path = list(data['file_name'])\n",
        "    name = list(data['file_name'])\n",
        "    label = list(data['misogynous'])\n",
        "\n",
        "    # Initializing lists for storing processed data\n",
        "    text_features, image_features, l, Name, v, text_descriptions = [], [], [], [], [], []\n",
        "\n",
        "    for txt, img, L, n in tqdm(zip(text, img_path, label, name)):\n",
        "        try:\n",
        "            img = Image.open('/DATA/atul_2221cs20/jitendra/MAMI_2022_images/training_images/' + img)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "        img = torch.stack([compose(img).to(device)])\n",
        "\n",
        "        l.append(L)\n",
        "        Name.append(n)\n",
        "        text_descriptions.append(txt)  # Append the text description to the list\n",
        "\n",
        "        with torch.no_grad():\n",
        "            txt = clip.tokenize(txt,truncate=True).to(device)\n",
        "            temp_txt = clip_model.encode_text(txt).detach().cpu().numpy()\n",
        "\n",
        "#             temp_txt = model.forward(txt, tokenizer).detach().cpu().numpy()\n",
        "            text_features.append(temp_txt)\n",
        "\n",
        "            temp_img = clip_model.encode_image(img).detach().cpu().numpy()\n",
        "            image_features.append(temp_img)\n",
        "\n",
        "            del temp_txt\n",
        "            del temp_img\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        del img\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Return the text descriptions along with the other data\n",
        "    return text_features, image_features, l, Name, text_descriptions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d024455d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b4aa8805a7c34940ada9058920889a98"
          ]
        },
        "id": "d024455d",
        "outputId": "fa2fd996-1ad2-4507-fa4f-ba521e9e8f96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4aa8805a7c34940ada9058920889a98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#trial\n",
        "text_features, image_features, l, Name, text_descriptions = get_data(data.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363a4a58",
      "metadata": {
        "id": "363a4a58",
        "outputId": "6d7dda6a-e303-41b8-d314-ae7f2599923b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(len(text_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640623be",
      "metadata": {
        "id": "640623be",
        "outputId": "88bd24d3-4f25-4612-e130-3cffd9c819e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Milk Milk.zip', 'ROSES ARE RED']"
            ]
          },
          "execution_count": 294,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f321f4bd",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f75aaff657b4491391e37e8208dc9e4a"
          ]
        },
        "id": "f321f4bd",
        "outputId": "4e25f27c-b55b-4178-fa4e-d72d7b6675c3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f75aaff657b4491391e37e8208dc9e4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9941 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "outliers = []\n",
        "for names in tqdm(list(data['file_name'])):\n",
        "  #change the path according to your drive\n",
        "  if not os.path.exists('/DATA/atul_2221cs20/jitendra/MAMI_2022_images/training_images/'+names):\n",
        "    outliers.append(names)\n",
        "\n",
        "# data = data[~data['Name'].isin(outliers)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5a002c",
      "metadata": {
        "id": "ea5a002c",
        "outputId": "1a63209c-01e5-4851-a585-899a301d0b18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 296,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30281fc6",
      "metadata": {
        "id": "30281fc6"
      },
      "outputs": [],
      "source": [
        "class HatefulDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        # Assuming get_data now also returns text_descriptions\n",
        "        self.t_f, self.i_f, self.label, self.name, self.text_descriptions = get_data(data)\n",
        "\n",
        "        # Squeeze the arrays if necessary (assuming this is for removing unnecessary dimensions)\n",
        "        self.t_f = np.squeeze(np.asarray(self.t_f), axis=1)\n",
        "        self.i_f = np.squeeze(np.asarray(self.i_f), axis=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        # The length of the dataset is the length of the labels\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert idx from tensor to list if it's a tensor\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Extract data for the given index\n",
        "        name = self.name[idx]\n",
        "        label = self.label[idx]\n",
        "        T = self.t_f[idx, :]\n",
        "        I = self.i_f[idx, :]\n",
        "        text_description = self.text_descriptions[idx]  # Extract the text description\n",
        "\n",
        "        # Create a sample dictionary to return\n",
        "        sample = {\n",
        "            'label': label,\n",
        "            'processed_txt': T,\n",
        "            'processed_img': I,\n",
        "            'name': name,\n",
        "            'text_description': text_description  # Include the text description in the sample\n",
        "        }\n",
        "\n",
        "        return sample\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0449b64f",
      "metadata": {
        "id": "0449b64f"
      },
      "outputs": [],
      "source": [
        "#sample_dataset = HatefulDataset(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1beb1995",
      "metadata": {
        "id": "1beb1995"
      },
      "outputs": [],
      "source": [
        "#sample_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae241bb8",
      "metadata": {
        "id": "ae241bb8"
      },
      "outputs": [],
      "source": [
        "#len(sample_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e856a368",
      "metadata": {
        "id": "e856a368"
      },
      "outputs": [],
      "source": [
        "#torch.save(sample_dataset,'MAMI_train.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338cbc56",
      "metadata": {
        "id": "338cbc56"
      },
      "outputs": [],
      "source": [
        "sample_dataset_new= torch.load(\"MAMI_train.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c48749",
      "metadata": {
        "id": "86c48749"
      },
      "outputs": [],
      "source": [
        "#sample_dataset_new[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6f5a49",
      "metadata": {
        "id": "7e6f5a49"
      },
      "outputs": [],
      "source": [
        "#test= torch.load(\"MAMI_train.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de749a7",
      "metadata": {
        "id": "7de749a7",
        "outputId": "e0d4b9bc-a57c-4cdb-d69d-4e07211bbbe3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9941"
            ]
          },
          "execution_count": 305,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sample_dataset_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf806f46",
      "metadata": {
        "id": "bf806f46"
      },
      "outputs": [],
      "source": [
        "#loding test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a193de",
      "metadata": {
        "id": "e0a193de"
      },
      "outputs": [],
      "source": [
        "def get_data1(data_test):  # data = pd.read_csv(dataset_path)\n",
        "    # Extracting lists from the DataFrame\n",
        "    text = list(data_test['Text_Transcription'])\n",
        "    img_path = list(data_test['file_name'])\n",
        "    name = list(data_test['file_name'])\n",
        "    label = list(data_test['misogynous'])\n",
        "\n",
        "    # Initializing lists for storing processed data\n",
        "    text_features, image_features, l, Name, v, text_descriptions = [], [], [], [], [], []\n",
        "\n",
        "    for txt, img, L, n in tqdm(zip(text, img_path, label, name)):\n",
        "        try:\n",
        "            img = Image.open('/DATA/atul_2221cs20/jitendra/MAMI_2022_images/test_images/' + img)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "        img = torch.stack([compose(img).to(device)])\n",
        "\n",
        "        l.append(L)\n",
        "        Name.append(n)\n",
        "        text_descriptions.append(txt)  # Append the text description to the list\n",
        "\n",
        "        with torch.no_grad():\n",
        "            txt = clip.tokenize(txt,truncate=True).to(device)\n",
        "            temp_txt = clip_model.encode_text(txt).detach().cpu().numpy()\n",
        "\n",
        "#             temp_txt = model.forward(txt, tokenizer).detach().cpu().numpy()\n",
        "            text_features.append(temp_txt)\n",
        "\n",
        "            temp_img = clip_model.encode_image(img).detach().cpu().numpy()\n",
        "            image_features.append(temp_img)\n",
        "\n",
        "            del temp_txt\n",
        "            del temp_img\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        del img\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Return the text descriptions along with the other data\n",
        "    return text_features, image_features, l, Name, text_descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570c42bb",
      "metadata": {
        "id": "570c42bb"
      },
      "outputs": [],
      "source": [
        "# outliers1 = []\n",
        "# for names in tqdm(list(data['file_name'])):\n",
        "#   #change the path according to your drive\n",
        "#   if not os.path.exists('/DATA/atul_2221cs20/jitendra/MAMI_2022_images/test_images/'+names):\n",
        "#     outliers1.append(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efde154a",
      "metadata": {
        "id": "efde154a"
      },
      "outputs": [],
      "source": [
        "# outliers1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b10d7a5",
      "metadata": {
        "id": "6b10d7a5"
      },
      "outputs": [],
      "source": [
        "class HatefulDataset1(Dataset):\n",
        "    def __init__(self, data):\n",
        "        # Assuming get_data now also returns text_descriptions\n",
        "        self.t_f, self.i_f, self.label, self.name, self.text_descriptions = get_data1(data)\n",
        "\n",
        "        # Squeeze the arrays if necessary (assuming this is for removing unnecessary dimensions)\n",
        "        self.t_f = np.squeeze(np.asarray(self.t_f), axis=1)\n",
        "        self.i_f = np.squeeze(np.asarray(self.i_f), axis=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        # The length of the dataset is the length of the labels\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert idx from tensor to list if it's a tensor\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Extract data for the given index\n",
        "        name = self.name[idx]\n",
        "        label = self.label[idx]\n",
        "        T = self.t_f[idx, :]\n",
        "        I = self.i_f[idx, :]\n",
        "        text_description = self.text_descriptions[idx]  # Extract the text description\n",
        "\n",
        "        # Create a sample dictionary to return\n",
        "        sample = {\n",
        "            'label': label,\n",
        "            'processed_txt': T,\n",
        "            'processed_img': I,\n",
        "            'name': name,\n",
        "            'text_description': text_description  # Include the text description in the sample\n",
        "        }\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a01512",
      "metadata": {
        "id": "a2a01512"
      },
      "outputs": [],
      "source": [
        "#sample_dataset_t = HatefulDataset1(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ba5dd2",
      "metadata": {
        "id": "a1ba5dd2"
      },
      "outputs": [],
      "source": [
        "#torch.save(sample_dataset_t,'MAMI_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba88fa50",
      "metadata": {
        "id": "ba88fa50"
      },
      "outputs": [],
      "source": [
        "sample_dataset_test= torch.load(\"MAMI_test.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b82766d",
      "metadata": {
        "id": "9b82766d",
        "outputId": "b45f8ff1-ecf3-41db-939e-957f1f0c8d61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 314,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sample_dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca4e724",
      "metadata": {
        "id": "dca4e724",
        "outputId": "aad78f01-342e-40af-b294-39ae2291a58e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'label': 0,\n",
              " 'processed_txt': array([-2.7603e-02, -1.2413e-02, -9.5978e-03,  2.7002e-01,  2.6831e-01,\n",
              "         2.9199e-01, -3.6914e-01, -6.9922e-01, -3.4692e-01, -1.8616e-01,\n",
              "         2.7734e-01,  2.4304e-01,  3.1714e-01,  1.2321e-02, -6.6345e-02,\n",
              "        -1.7920e-01, -2.9663e-01, -9.5215e-02, -8.5754e-02, -1.8445e-01,\n",
              "        -1.0022e-01,  3.8738e-03,  3.1689e-01, -2.7069e-02, -9.8389e-02,\n",
              "         4.7493e-03,  2.2675e-02, -2.7734e-01, -1.3989e-01,  2.5781e-01,\n",
              "        -9.4360e-02, -8.2397e-02,  1.8762e-01, -1.3260e-02,  2.7368e-01,\n",
              "        -3.9520e-02, -5.2094e-02,  6.3049e-02, -1.0437e-01, -6.3916e-01,\n",
              "         1.6138e-01, -1.7029e-01,  2.6001e-01,  3.1689e-01,  1.7737e-01,\n",
              "         5.6348e-01, -1.9299e-01, -1.3660e-01, -2.9761e-01, -1.9604e-01,\n",
              "         1.1432e-01, -2.7246e-01,  1.1139e-01,  2.3657e-01, -2.3389e-01,\n",
              "         2.4084e-01,  1.1023e-01,  3.5547e-01,  6.8237e-02,  5.8691e-01,\n",
              "         3.2617e-01,  1.5320e-01,  7.1582e-01, -1.1810e-01, -7.3059e-02,\n",
              "         2.5940e-03, -9.4461e-04,  5.0195e-01, -6.6528e-02, -2.8955e-01,\n",
              "         1.8359e-01,  1.2830e-01,  1.3208e-01, -1.2006e-01,  3.9746e-01,\n",
              "        -1.8066e-01, -1.4648e-01, -7.1777e-02, -5.2795e-02, -2.3059e-01,\n",
              "        -2.9785e-01,  3.8013e-01, -2.6196e-01,  5.2295e-01,  1.4824e-02,\n",
              "        -5.3925e-02, -1.1218e-01, -3.3966e-02, -3.8989e-01,  1.7014e-02,\n",
              "        -6.0608e-02, -2.3047e-01, -1.2041e+00, -4.2310e-01, -1.2103e-01,\n",
              "        -2.7634e-02, -1.6760e-01, -3.9868e-01,  1.4368e-01,  3.3789e-01,\n",
              "        -2.8979e-01,  2.4365e-01,  1.6495e-02, -7.4158e-02, -4.5386e-01,\n",
              "        -1.9324e-01,  5.2223e-03, -1.4978e-01, -1.6144e-02, -1.2512e-01,\n",
              "         2.2546e-01,  1.8311e-01, -5.3070e-02, -2.5220e-01,  2.7002e-01,\n",
              "        -5.3131e-02, -2.3145e-01,  2.8763e-02, -2.6782e-01,  3.1104e-01,\n",
              "        -2.6733e-01, -6.6284e-02, -8.2764e-02, -1.3599e-01, -5.4840e-02,\n",
              "        -2.0593e-01, -1.4429e-01,  1.1420e-01,  1.5344e-01,  9.9121e-02,\n",
              "        -1.7810e-01, -8.0383e-02,  1.8173e-02,  5.6172e+00, -9.2224e-02,\n",
              "        -1.2598e-01,  3.8257e-01, -3.3667e-01, -1.5967e-01,  2.7588e-01,\n",
              "        -1.7883e-01, -5.3345e-02,  9.5520e-02, -5.2734e-01, -1.9275e-01,\n",
              "         1.8201e-01,  3.1226e-01, -2.7686e-01,  3.7061e-01, -1.1359e-01,\n",
              "         1.9287e-01,  1.5149e-01,  1.0413e-01, -3.9673e-01, -1.2573e-01,\n",
              "        -4.0771e-01, -2.9248e-01,  1.7871e-01, -5.9235e-02,  1.0120e-01,\n",
              "         2.9150e-01, -1.0907e-01, -1.3074e-01,  2.5562e-01,  3.8232e-01,\n",
              "         2.3071e-01, -1.4844e-01, -2.0459e-01, -8.9539e-02,  3.6438e-02,\n",
              "         9.2712e-02, -1.5747e-01,  3.9282e-01, -2.8369e-01,  8.2275e-02,\n",
              "         2.5488e-01, -1.8127e-02, -1.3443e-02, -1.3818e-01, -2.3022e-01,\n",
              "        -1.0910e-02,  1.9092e-01, -1.3379e-01,  3.1812e-01,  1.4929e-01,\n",
              "        -1.0425e-01, -1.2659e-01,  1.9128e-01, -4.9365e-01,  4.4873e-01,\n",
              "        -4.5630e-01,  4.7314e-01, -4.9896e-02, -4.0161e-01, -1.8372e-01,\n",
              "        -1.2634e-02,  2.4060e-01, -2.5620e-02,  7.6477e-02,  1.0864e-01,\n",
              "        -5.0537e-01,  5.8960e-02,  2.8320e-01,  8.7967e-03,  2.5195e-01,\n",
              "         1.6772e-01,  2.5464e-01, -5.4291e-02, -1.1310e-01, -3.1689e-01,\n",
              "         3.1079e-01,  1.2262e-01,  5.3271e-01, -4.5605e-01, -2.7466e-01,\n",
              "         1.4819e-01,  1.5662e-01,  3.4546e-02, -6.0913e-02, -1.5674e-01,\n",
              "         1.7285e-01,  2.3230e-01, -1.3599e-01,  2.8247e-01,  3.4521e-01,\n",
              "         5.6763e-02,  5.4779e-03,  2.9175e-02, -4.8340e-02,  5.0244e-01,\n",
              "        -1.8652e-01,  1.7322e-01,  4.3481e-01,  5.6299e-01, -1.3013e-01,\n",
              "         1.7322e-01,  5.7275e-01, -6.8176e-02, -4.7314e-01, -3.0884e-01,\n",
              "        -3.0005e-01,  2.0361e-01,  1.7493e-01,  8.6731e-02, -3.9478e-01,\n",
              "         3.4302e-01, -1.6870e-01,  3.6182e-01, -5.9631e-02, -7.4829e-02,\n",
              "        -7.8308e-02,  2.9541e-01,  1.1584e-01,  4.3457e-01,  1.3879e-01,\n",
              "         4.0601e-01, -1.6284e-01, -1.8311e-01, -3.0957e-01,  1.0345e-01,\n",
              "         2.9248e-01,  1.8701e-01,  4.0161e-01, -2.1561e-02,  4.9219e-01,\n",
              "        -1.1890e-01,  1.7688e-01,  2.1103e-02,  6.5796e-02, -1.7932e-01,\n",
              "         2.8418e-01,  4.0698e-01,  1.3049e-01, -1.1673e-02,  4.1626e-02,\n",
              "         3.7549e-01, -1.7761e-02,  1.5857e-01,  4.0381e-01,  1.3391e-01,\n",
              "         3.3130e-01, -5.9937e-02,  8.0185e-03,  1.3623e-01,  2.1997e-01,\n",
              "         1.2030e-01, -5.1666e-02, -8.9905e-02, -2.9810e-01, -6.2988e-02,\n",
              "        -3.7280e-01,  2.1301e-01, -1.5100e-01,  1.7993e-01, -8.6243e-02,\n",
              "         2.0166e-01,  4.9103e-02,  2.1191e-01,  2.7490e-01,  3.1470e-01,\n",
              "        -5.3635e-03,  1.7957e-01,  4.5624e-02,  3.3661e-02,  3.2788e-01,\n",
              "         3.0005e-01, -1.2268e-02,  4.2511e-02,  1.1847e-01, -2.9639e-01,\n",
              "         5.4688e-01, -1.3806e-01,  5.6211e+00, -1.6467e-01,  1.9592e-01,\n",
              "         2.0966e-02,  3.2886e-01,  5.7251e-02, -6.3660e-02, -4.8340e-02,\n",
              "         2.9053e-02,  4.5837e-02, -4.4531e-01,  8.5999e-02,  1.6968e-01,\n",
              "         7.2815e-02,  1.0376e-01, -3.0176e-01, -4.0802e-02, -9.2188e-01,\n",
              "        -2.4634e-01,  1.5515e-01, -8.9905e-02,  9.5215e-02,  4.6783e-02,\n",
              "         1.3062e-01, -5.8365e-03, -1.2830e-01,  2.9373e-02, -2.0044e-01,\n",
              "         2.5659e-01,  2.1985e-01, -1.7563e-02,  6.7472e-04,  9.7473e-02,\n",
              "         1.4246e-01, -1.5112e-01, -1.2347e-01, -1.4404e-01,  1.5732e-02,\n",
              "         3.9795e-01,  2.9736e-01, -7.4707e-02,  3.1071e-03,  2.1667e-01,\n",
              "         2.4634e-01,  1.3733e-01,  2.2717e-01, -4.1046e-02, -1.7505e-01,\n",
              "         1.9666e-01, -2.1912e-01, -2.6904e-01,  7.9529e-02, -1.8298e-01,\n",
              "        -3.0273e-01,  8.7708e-02,  8.8135e-02,  5.7764e-01, -1.7761e-01,\n",
              "         5.3650e-02, -1.0919e-01, -1.0211e-01,  1.8628e-01,  2.4200e-02,\n",
              "        -1.0352e-01, -9.3506e-02,  5.0842e-02, -4.4019e-01,  2.8503e-02,\n",
              "         1.7395e-01,  1.9019e-01, -2.3096e-01, -2.3413e-01, -5.1270e-02,\n",
              "         2.0996e-01,  1.0889e-01,  2.9346e-01, -9.5337e-02, -3.8574e-02,\n",
              "        -3.2349e-01, -3.0537e-03,  3.2739e-01,  4.8950e-02,  2.7863e-02,\n",
              "         1.0071e-01, -1.1711e-03, -4.1840e-02, -1.4954e-01,  3.8208e-02,\n",
              "         2.5488e-01,  3.6987e-02, -1.6403e-02,  1.8994e-01,  5.2246e-01,\n",
              "         5.8556e-03, -1.1700e-01, -3.3765e-01, -7.2388e-02,  1.9165e-01,\n",
              "        -5.2002e-02,  1.2323e-01,  8.1970e-02, -3.8330e-01, -2.9980e-01,\n",
              "        -1.2527e-02,  5.5322e-01, -4.3262e-01,  8.4900e-02,  5.1025e-02,\n",
              "        -1.3196e-01, -2.1667e-01,  2.4841e-01,  2.3840e-01, -2.0081e-01,\n",
              "         1.2500e-01,  5.7666e-01,  2.2009e-01,  2.2156e-01, -7.8247e-02,\n",
              "        -2.2534e-01, -5.7945e-03,  1.7358e-01,  2.6562e-01,  8.9661e-02,\n",
              "         7.1826e-01, -5.8398e-01,  3.9038e-01, -7.2876e-02, -5.6543e-01,\n",
              "        -3.1235e-02,  7.6025e-01,  2.1948e-01,  4.0356e-01,  1.7261e-01,\n",
              "        -2.0874e-01,  3.3643e-01,  3.4717e-01,  2.7930e-01, -4.5435e-01,\n",
              "        -1.5210e-01, -2.5146e-01,  4.8523e-03,  1.7654e-02, -2.1729e-01,\n",
              "        -5.7275e-01, -1.0712e-01,  1.1032e-02,  3.2440e-02,  3.5669e-01,\n",
              "         4.0430e-01,  2.0715e-01,  1.1456e-01,  1.1456e-01, -5.9814e-01,\n",
              "        -3.1836e-01, -6.0938e-01,  2.6294e-01,  2.2510e-01, -3.4058e-01,\n",
              "        -2.7441e-01,  4.2084e-02,  7.5989e-02,  1.1224e-01, -2.2449e-01,\n",
              "         7.1533e-02, -2.1692e-01,  1.7590e-01,  3.9819e-01,  3.7727e-03,\n",
              "        -2.5391e-01,  1.0022e-01,  2.2995e-02,  5.8154e-01, -2.1265e-01,\n",
              "         1.4453e-01, -1.3586e-01, -1.2091e-01, -5.0098e-01,  2.0801e-01,\n",
              "         2.1753e-01, -2.1729e-01,  2.7563e-01,  7.2559e-01,  4.0991e-01,\n",
              "         1.1426e-01, -1.5625e-01,  1.5112e-01, -1.4380e-01, -1.7090e-01,\n",
              "         6.8994e-01,  3.7866e-01, -1.9641e-01, -1.8726e-01,  4.5557e-01,\n",
              "         3.2928e-02, -2.2083e-01, -6.8054e-02,  4.8682e-01,  1.6675e-01,\n",
              "         2.0850e-01,  2.1622e-02], dtype=float16),\n",
              " 'processed_img': array([-3.6328e-01, -3.2275e-01, -6.5979e-02, -5.9631e-02,  4.3701e-01,\n",
              "        -3.7305e-01, -2.9224e-01,  1.7590e-01, -3.1567e-01, -4.1046e-02,\n",
              "         5.9180e-01,  4.2822e-01,  8.5791e-01,  2.3254e-01,  1.2695e-01,\n",
              "        -9.1309e-02, -1.1639e-01,  2.1362e-01,  1.4221e-01, -2.5073e-01,\n",
              "         3.3984e-01,  5.2582e-02,  1.4429e-01, -1.8982e-01, -7.2754e-01,\n",
              "        -8.1665e-02, -1.2207e-01, -3.3887e-01, -3.0054e-01,  4.2334e-01,\n",
              "        -2.1570e-01, -8.9050e-02, -2.1448e-01,  2.6382e-02, -3.9124e-02,\n",
              "         1.5039e-01, -4.3762e-02,  4.5239e-01, -4.4971e-01, -1.9570e+00,\n",
              "         1.4868e-01, -2.9346e-01,  1.5894e-01,  2.4573e-01, -1.9519e-01,\n",
              "         1.6914e+00,  3.4717e-01,  7.6294e-02, -4.2188e-01,  2.8149e-01,\n",
              "        -5.9180e-01,  1.1688e-01,  1.6968e-01,  6.7017e-02,  9.2407e-02,\n",
              "         3.9185e-02, -4.8859e-02,  3.9307e-01,  5.3027e-01, -7.6523e-03,\n",
              "        -1.1768e-01, -1.9153e-01,  2.4573e-01, -1.1157e-01, -6.2317e-02,\n",
              "         2.0593e-01, -1.6882e-01,  1.2412e+00, -9.6008e-02, -1.2825e-02,\n",
              "        -2.0056e-01, -7.2388e-02, -2.1408e-02,  2.9297e-01, -1.8347e-01,\n",
              "        -2.4695e-01, -1.0910e-03, -3.2080e-01, -2.7075e-01,  3.8843e-01,\n",
              "        -1.7029e-01,  3.8647e-01,  3.3173e-02,  6.0547e-01, -3.2013e-02,\n",
              "        -3.5004e-02, -1.4694e-02, -4.5068e-01,  4.3750e-01, -3.6011e-02,\n",
              "        -2.1988e-02, -4.2944e-01, -5.8320e+00,  7.6611e-01, -1.4214e-02,\n",
              "         2.5122e-01,  9.3750e-02, -1.8387e-02, -8.2715e-01,  3.8818e-01,\n",
              "        -1.1157e-01,  6.7932e-02, -1.4355e-01, -9.9182e-02, -8.6328e-01,\n",
              "         1.5320e-01,  3.1641e-01,  2.0654e-01, -5.3864e-02,  5.5298e-02,\n",
              "         1.8005e-01,  5.3125e-01,  1.2683e-01, -4.5459e-01, -3.5913e-01,\n",
              "        -2.6367e-01, -2.6733e-01, -2.4979e-02,  1.1359e-01, -2.9199e-01,\n",
              "        -3.1708e-02,  3.1567e-01, -3.2764e-01,  2.7905e-01, -3.9551e-01,\n",
              "         2.9004e-01, -3.5742e-01, -6.3293e-02, -2.4304e-01, -1.1737e-01,\n",
              "         2.6367e-01,  3.1860e-01, -5.7190e-02,  8.9062e-01, -2.2083e-01,\n",
              "         1.1401e-01, -6.5002e-02,  2.8149e-01,  1.4087e-01,  1.6443e-01,\n",
              "         2.0496e-01, -7.7026e-02, -3.5767e-02,  5.3467e-01, -5.6592e-01,\n",
              "        -2.5978e-03,  3.8550e-01, -3.6157e-01, -4.8767e-02,  2.6855e-01,\n",
              "         1.9983e-01,  2.6221e-01,  9.8633e-01, -1.0980e-01,  2.0471e-01,\n",
              "        -1.5442e-01, -2.2888e-01, -5.6104e-01, -3.6377e-01,  4.0991e-01,\n",
              "        -2.5146e-01, -1.8250e-01, -1.5771e-01,  4.2496e-03,  5.9753e-02,\n",
              "        -7.1045e-01,  1.6980e-01,  2.0483e-01,  2.2415e-02, -1.0529e-01,\n",
              "        -2.4182e-01, -6.8750e-01, -2.1957e-02, -2.0020e-01, -3.7354e-01,\n",
              "         2.1472e-01, -1.6084e+00, -3.5278e-01, -8.9990e-01, -4.4019e-01,\n",
              "         2.0471e-01, -2.6123e-01, -3.2495e-01,  1.8616e-01, -1.0353e-02,\n",
              "         1.2497e-02,  8.4457e-03,  1.5295e-01, -4.1431e-01, -1.7004e-01,\n",
              "        -2.1045e-01,  4.4019e-01,  4.8999e-01,  2.0630e-01, -7.3926e-01,\n",
              "         7.5928e-02,  6.2805e-02,  1.3794e-02, -8.2703e-02,  1.9211e-02,\n",
              "         1.4624e-01, -2.8687e-01,  9.6436e-02, -2.0154e-01,  8.5449e-01,\n",
              "         4.7656e-01, -5.2567e-03,  2.9175e-01,  1.5295e-01, -2.6050e-01,\n",
              "        -3.3569e-01,  3.2471e-01,  4.4263e-01, -8.8745e-02, -3.2935e-01,\n",
              "         4.2578e-01,  5.7275e-01,  7.4829e-02, -1.6528e-01, -2.0337e-01,\n",
              "         4.3701e-01,  5.9668e-01, -4.1284e-01,  2.4060e-01,  1.3879e-01,\n",
              "         1.3257e-01, -3.5840e-01, -3.3203e-01, -6.3293e-02,  1.7200e-01,\n",
              "        -6.2402e-01,  9.0576e-02,  1.6876e-02,  7.4902e-01, -5.4199e-01,\n",
              "        -8.1006e-01,  3.0151e-01, -1.5662e-01, -2.1167e-01, -1.5735e-01,\n",
              "         4.7485e-02, -4.0674e-01, -8.0078e-02,  2.9858e-01,  2.2510e-01,\n",
              "         5.5518e-01,  1.5454e-01,  1.8518e-01,  4.4751e-01,  8.6865e-01,\n",
              "        -7.9443e-01,  5.3906e-01, -1.4819e-01,  2.5977e-01, -4.2480e-02,\n",
              "         4.9365e-01, -1.4587e-01,  4.0186e-01, -8.7061e-01, -1.6919e-01,\n",
              "         1.6687e-01,  5.2704e-02,  3.2690e-01,  9.6875e-01,  5.2588e-01,\n",
              "         1.0635e-02,  4.6094e-01, -2.4017e-02, -2.8833e-01,  3.7524e-01,\n",
              "         3.7793e-01,  2.4561e-01, -8.4473e-02,  2.6514e-01,  2.0850e-01,\n",
              "         1.0510e-01, -1.3684e-01, -2.8003e-01,  1.8372e-02,  2.0972e-01,\n",
              "        -2.5122e-01, -2.5708e-01, -3.0249e-01, -3.0835e-01,  2.0947e-01,\n",
              "         3.5693e-01, -5.9424e-01, -4.1064e-01, -3.0493e-01, -6.2286e-02,\n",
              "         5.7404e-02, -2.4052e-03, -2.9492e-01, -6.6956e-02, -2.2510e-01,\n",
              "         1.2863e-02,  1.0284e-02, -1.1469e-01,  1.3199e-02,  4.8169e-01,\n",
              "        -3.0884e-02, -2.8931e-01, -1.3867e-01, -6.2622e-02,  3.7427e-01,\n",
              "         4.7754e-01, -1.5100e-01,  3.8513e-02,  2.7686e-01,  4.4067e-01,\n",
              "         3.7573e-01, -2.5952e-01,  8.9062e-01, -3.1128e-01,  3.5254e-01,\n",
              "        -8.7463e-02,  2.7197e-01,  1.5479e-01, -2.8540e-01, -1.7957e-01,\n",
              "        -3.2251e-01,  4.5923e-01,  1.4807e-01,  2.6636e-01, -4.2480e-01,\n",
              "         2.7441e-01,  4.6826e-01, -1.7188e-01,  3.2007e-01, -4.3823e-01,\n",
              "        -1.6321e-01,  4.0332e-01,  4.3140e-01,  2.1826e-01, -2.5122e-01,\n",
              "         8.8562e-02,  3.6230e-01,  1.6187e-01, -2.0947e-01, -5.9277e-01,\n",
              "         4.2798e-01,  5.9723e-02, -4.3115e-01,  3.5303e-01, -1.0095e-01,\n",
              "         1.6882e-01,  1.2708e-01, -3.9062e-01,  5.0195e-01,  8.7500e-01,\n",
              "         5.2539e-01, -3.0786e-01,  3.4229e-01,  4.4409e-01, -4.8755e-01,\n",
              "         4.3579e-01, -4.7412e-01, -1.5332e-01,  3.5620e-01, -1.3770e-01,\n",
              "         8.5303e-01,  4.4189e-01,  1.4893e-01,  3.1348e-01, -8.4912e-01,\n",
              "        -2.4353e-01,  1.3527e-02,  4.5947e-01,  4.3976e-02,  1.9482e-01,\n",
              "         3.2745e-02,  3.5620e-01, -5.4199e-01, -3.7158e-01,  2.4500e-01,\n",
              "        -6.0254e-01,  9.1992e-01,  1.5015e-01, -2.4744e-01, -2.8979e-01,\n",
              "        -6.1859e-02, -3.6353e-01, -1.0205e-01, -5.8643e-01, -1.6370e-01,\n",
              "        -3.9380e-01,  5.3619e-02,  7.6318e-01, -4.6948e-01, -1.2559e+00,\n",
              "        -6.4404e-01, -2.9077e-01,  1.0307e-02,  1.4893e-01,  8.0185e-03,\n",
              "         7.3181e-02, -1.0971e-02, -1.3818e-01, -2.6172e-01, -4.8120e-01,\n",
              "         2.1545e-01,  2.6416e-01, -1.1650e+00,  1.0193e-01, -2.3291e-01,\n",
              "         3.7323e-02,  2.4011e-01,  4.0894e-01,  1.8665e-01,  3.7939e-01,\n",
              "        -5.1575e-02, -3.6816e-01,  6.3184e-01,  1.8994e-01, -1.1859e-01,\n",
              "         1.9202e-01,  6.4307e-01, -9.2010e-03,  2.2351e-01, -5.4248e-01,\n",
              "         3.8281e-01,  2.9224e-01,  4.6191e-01,  9.3848e-01, -2.3560e-02,\n",
              "         2.4353e-01,  1.5698e-01,  2.8003e-01,  2.6831e-01, -1.2067e-01,\n",
              "         2.5848e-02,  3.8623e-01,  7.2168e-01, -1.4490e-01,  2.2778e-01,\n",
              "         3.3252e-01, -2.3792e-01,  3.3984e-01, -4.3701e-01, -1.0681e-01,\n",
              "        -5.1025e-01,  3.1592e-01,  8.2947e-02,  4.2456e-01, -4.3701e-01,\n",
              "        -1.9763e-01,  5.4779e-02, -2.8833e-01,  4.3701e-01,  1.3416e-01,\n",
              "        -6.9580e-01,  3.5693e-01,  1.2878e-01, -1.0767e-01,  2.0981e-02,\n",
              "         2.3666e-02,  5.5469e-01, -8.9294e-02,  3.5889e-01,  4.2139e-01,\n",
              "         2.5952e-01,  6.8298e-02,  1.8225e-01,  3.2861e-01,  1.9946e-01,\n",
              "         7.3633e-01, -2.8394e-01,  4.3628e-01, -1.3123e-01, -5.2063e-02,\n",
              "        -1.1284e-02,  4.1138e-02, -9.6069e-02, -2.5684e-01, -1.1163e-01,\n",
              "         2.2180e-01, -2.1582e-01,  7.6318e-01,  6.8481e-02,  1.7288e-02,\n",
              "        -6.2042e-02, -5.1788e-02,  4.2822e-01,  2.5830e-01, -3.0371e-01,\n",
              "         2.0630e-01, -4.6826e-01, -1.0187e-01,  2.3401e-01,  2.7563e-01,\n",
              "         2.0984e-01, -5.2588e-01,  5.1611e-01, -1.7798e-01, -1.1676e-01,\n",
              "         1.2561e-01,  1.1768e-01, -2.5708e-01, -6.5491e-02, -1.4856e-01,\n",
              "         5.6836e-01,  1.8225e-01, -4.4287e-01, -5.0781e-01,  2.7344e-01,\n",
              "         1.7261e-01, -5.5859e-01, -1.1743e-01, -2.3026e-02, -9.3323e-02,\n",
              "         6.0822e-02,  6.5125e-02], dtype=float16),\n",
              " 'name': '15002.jpg',\n",
              " 'text_description': 'How limagined myself as a Teacher...... How I feel as a Teacher... panda cont'}"
            ]
          },
          "execution_count": 315,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_dataset_test[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65475b0e",
      "metadata": {
        "id": "65475b0e"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687028e0",
      "metadata": {
        "id": "687028e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MFB(nn.Module):  #multi-modal feature fusion between image and question features.\n",
        "\n",
        "    def __init__(self,img_feat_size, ques_feat_size, is_first, MFB_K, MFB_O, DROPOUT_R):\n",
        "        super(MFB, self).__init__()  # calls the constructor of the parent class\n",
        "        #store hyperparameters\n",
        "        #self.__C = __C\n",
        "        self.MFB_K = MFB_K #hyperparameter for pooling\n",
        "        self.MFB_O = MFB_O  #hyperparameter for output dimension\n",
        "        self.DROPOUT_R = DROPOUT_R\n",
        "        #print(MFB_K)\n",
        "\n",
        "        #img_feat_size = 5 (dimension of image feature vectors)\n",
        "\n",
        "        self.is_first = is_first  #Stores a boolean flag indicating if this is the first MFB module\n",
        "        self.proj_i = nn.Linear(img_feat_size, MFB_K * MFB_O) #Creates a linear layer for image feature projection\n",
        "\n",
        "        self.proj_q = nn.Linear(ques_feat_size, MFB_K * MFB_O) #Creates a linear layer for question feature projection\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_R) #Creates a dropout layer for regularization\n",
        "        self.pool = nn.AvgPool1d(MFB_K, stride = MFB_K) #Creates an average pooling layer for dimensionality reduction\n",
        "        #print(self.pool)\n",
        " #self.MFB = MFB(512, 38400, True, 256, 64, 0.1)\n",
        "    def forward(self, img_feat, ques_feat, exp_in=1):  #Defines how data flows through the module\n",
        "        '''\n",
        "            img_feat.size() -> (N, C, img_feat_size)    C = 1 or 100\n",
        "            ques_feat.size() -> (N, 1, ques_feat_size)\n",
        "            z.size() -> (N, C, MFB_O)\n",
        "            exp_out.size() -> (N, C, K*O)\n",
        "        '''\n",
        "\n",
        "        batch_size = img_feat.shape[0]\n",
        "        #print(img_feat.shape)\n",
        "        #print(img_feat.shape[0])\n",
        "        img_feat = self.proj_i(img_feat)#Projects image features using self.proj_i (output shape: (N, C, K*O))\n",
        "\n",
        "        #print(ques_feat.shape)\n",
        "        #print(img_feat.size())\n",
        "        ques_feat = self.proj_q(ques_feat)              # (N, 1, K*O)\n",
        "        #print(ques_feat.size())\n",
        "\n",
        "       # ques_feat = ques_feat.transpose(0, 1)\n",
        "\n",
        "\n",
        "\n",
        "        exp_out = img_feat * ques_feat             # (N, C, K*O)\n",
        "        #print(exp_out)\n",
        "       # print(exp_out.size())\n",
        "        exp_out = self.dropout(exp_out) if self.is_first else self.dropout(exp_out * exp_in)     # (N, C, K*O)\n",
        "        #print(exp_out)\n",
        "       # print(exp_out.size())\n",
        "        z = self.pool(exp_out) * self.MFB_K         # (N, C, O)\n",
        "\n",
        "\n",
        "        z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n",
        "        ##print(z)\n",
        "\n",
        "        z = F.normalize(z.view(batch_size, -1))         # (N, C*O)\n",
        "\n",
        "        z = z.view(batch_size, -1, self.MFB_O)      # (N, C, O)\n",
        "        #print(z)\n",
        "#         print(z.view(batch_size, -1))\n",
        "#         print(z)\n",
        "#         print(z.size())\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ca7f62",
      "metadata": {
        "id": "34ca7f62"
      },
      "outputs": [],
      "source": [
        "#torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3bd949",
      "metadata": {
        "id": "ff3bd949"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "t_p,v_p = torch.utils.data.random_split(sample_dataset_new,[9000,941])\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "t_p,te_p1 = torch.utils.data.random_split(t_p,[7000,2000])\n",
        "\n",
        "te_p=sample_dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a3e545",
      "metadata": {
        "id": "41a3e545",
        "outputId": "af0252fd-db4c-4421-ab18-4ba0be1654b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 320,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(te_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca0d1ae",
      "metadata": {
        "id": "5ca0d1ae",
        "outputId": "e6340ad7-9570-466f-d509-1d70b7ba914f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "execution_count": 321,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_p[1][\"processed_img\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366ffe3d",
      "metadata": {
        "id": "366ffe3d"
      },
      "outputs": [],
      "source": [
        "#  mask_and_predict(\"hi  how are you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f97c35",
      "metadata": {
        "id": "c7f97c35"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2ca703",
      "metadata": {
        "id": "1b2ca703"
      },
      "outputs": [],
      "source": [
        "#       logit_offen= self.forward(txt,img)\n",
        "#       pro=torch.sigmoid(logit_offen)\n",
        "#       print(pro)\n",
        "#       predicted_labels = torch.argmax(pro, dim=1)\n",
        "#       print(predicted_labels.tolist )\n",
        "\n",
        "\n",
        "#       loss = self.cross_entropy_loss(logit_offen, lab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21336eb7",
      "metadata": {
        "id": "21336eb7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9936af",
      "metadata": {
        "id": "5c9936af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3847b33",
      "metadata": {
        "id": "e3847b33"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b74b877",
      "metadata": {
        "id": "4b74b877"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd0024fd",
      "metadata": {
        "id": "cd0024fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987baaac",
      "metadata": {
        "id": "987baaac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f43b46",
      "metadata": {
        "id": "41f43b46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3990777d",
      "metadata": {
        "id": "3990777d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12bacc0c",
      "metadata": {
        "id": "12bacc0c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3852b4",
      "metadata": {
        "id": "6c3852b4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f6d739",
      "metadata": {
        "id": "62f6d739"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b59660",
      "metadata": {
        "id": "b3b59660"
      },
      "outputs": [],
      "source": [
        "print(len(contours))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3421812d",
      "metadata": {
        "id": "3421812d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7e98a9",
      "metadata": {
        "id": "6b7e98a9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eccdfc04",
      "metadata": {
        "id": "eccdfc04"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c4679e",
      "metadata": {
        "id": "b3c4679e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5722a37",
      "metadata": {
        "id": "e5722a37"
      },
      "outputs": [],
      "source": [
        "#!pip install transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc409a0f",
      "metadata": {
        "id": "fc409a0f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8337431a",
      "metadata": {
        "id": "8337431a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c99f2b2",
      "metadata": {
        "id": "0c99f2b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0d2c0c",
      "metadata": {
        "id": "2e0d2c0c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130732b3",
      "metadata": {
        "id": "130732b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4225d8e4",
      "metadata": {
        "id": "4225d8e4"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccee5e4",
      "metadata": {
        "id": "3ccee5e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b286bf79",
      "metadata": {
        "id": "b286bf79"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45cbf545",
      "metadata": {
        "id": "45cbf545"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e4de5ab",
      "metadata": {
        "id": "5e4de5ab"
      },
      "outputs": [],
      "source": [
        "#Now we will create a code for applications\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d114220",
      "metadata": {
        "id": "3d114220"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61ef2d8",
      "metadata": {
        "id": "d61ef2d8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8260f698",
      "metadata": {
        "id": "8260f698"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695e5044",
      "metadata": {
        "id": "695e5044"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f9b815",
      "metadata": {
        "id": "f2f9b815"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2f6439",
      "metadata": {
        "id": "fe2f6439"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc7cc2c",
      "metadata": {
        "id": "1bc7cc2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f386d92",
      "metadata": {
        "id": "3f386d92"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a22f05d",
      "metadata": {
        "id": "2a22f05d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334d7946",
      "metadata": {
        "id": "334d7946"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd1e09d",
      "metadata": {
        "scrolled": true,
        "id": "edd1e09d"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8581c60",
      "metadata": {
        "id": "d8581c60"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fc66fa",
      "metadata": {
        "id": "14fc66fa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb051d9",
      "metadata": {
        "id": "4bb051d9"
      },
      "outputs": [],
      "source": [
        "# HERE WE APPLY ATTENTION USING CLIP MODEL AND BLUR REQUIRED REGION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfcaa4ae",
      "metadata": {
        "id": "cfcaa4ae"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "#@markdown Please execute this cell by pressing the _Play_ button\n",
        "#@markdown on the left.\n",
        "\n",
        "#@markdown **Note**: This installs the software on the Colab\n",
        "#@markdown notebook in the cloud and not on your computer.\n",
        "\n",
        "!pip install ftfy regex tqdm matplotlib opencv-python scipy scikit-image\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "from PIL import Image\n",
        "from scipy.ndimage import filters\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4a0a7b",
      "metadata": {
        "id": "ae4a0a7b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0262317",
      "metadata": {
        "id": "a0262317"
      },
      "outputs": [],
      "source": [
        "#@title GradCAM: Gradient-weighted Class Activation Mapping\n",
        "\n",
        "#@markdown Our gradCAM implementation registers a forward hook\n",
        "#@markdown on the model at the specified layer. This allows us\n",
        "#@markdown to save the intermediate activations and gradients\n",
        "#@markdown at that layer.\n",
        "\n",
        "#@markdown To visualize which parts of the image activate for\n",
        "#@markdown a given caption, we use the caption as the target\n",
        "#@markdown label and backprop through the network using the\n",
        "#@markdown image as the input.\n",
        "#@markdown In the case of CLIP models with resnet encoders,\n",
        "#@markdown we save the activation and gradients at the\n",
        "#@markdown layer before the attention pool, i.e., layer4.\n",
        "#@title Helper functions\n",
        "\n",
        "#@markdown Some helper functions for overlaying heatmaps on top\n",
        "#@markdown of images and visualizing with matplotlib.\n",
        "\n",
        "def normalize(x: np.ndarray) -> np.ndarray:\n",
        "    # Normalize to [0, 1].\n",
        "    x = x - x.min()\n",
        "    if x.max() > 0:\n",
        "        x = x / x.max()\n",
        "    return x\n",
        "\n",
        "# Modified from: https://github.com/salesforce/ALBEF/blob/main/visualization.ipynb\n",
        "def getAttMap(img, attn_map, blur=True):\n",
        "    if blur:\n",
        "        attn_map = filters.gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
        "    attn_map = normalize(attn_map)\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
        "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
        "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
        "    return attn_map\n",
        "\n",
        "def viz_attn(img, attn_map, blur=True):\n",
        "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(img)\n",
        "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()\n",
        "    p=getAttMap(img, attn_map, blur)\n",
        "    print('p',p.shape)\n",
        "\n",
        "    return p\n",
        "\n",
        "def load_image(img_path, resize=None):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    if resize is not None:\n",
        "        image = image.resize((resize, resize))\n",
        "    return np.asarray(image).astype(np.float32) / 255.\n",
        "\n",
        "class Hook:\n",
        "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
        "\n",
        "    def __init__(self, module: nn.Module):\n",
        "        self.data = None\n",
        "        self.hook = module.register_forward_hook(self.save_grad)\n",
        "\n",
        "    def save_grad(self, module, input, output):\n",
        "        self.data = output\n",
        "        output.requires_grad_(True)\n",
        "        output.retain_grad()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        self.hook.remove()\n",
        "\n",
        "    @property\n",
        "    def activation(self) -> torch.Tensor:\n",
        "        return self.data\n",
        "\n",
        "    @property\n",
        "    def gradient(self) -> torch.Tensor:\n",
        "        return self.data.grad\n",
        "\n",
        "\n",
        "# Reference: https://arxiv.org/abs/1610.02391\n",
        "def gradCAM(\n",
        "    model: nn.Module,\n",
        "    input: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    layer: nn.Module\n",
        ") -> torch.Tensor:\n",
        "    # Zero out any gradients at the input.\n",
        "    if input.grad is not None:\n",
        "        input.grad.data.zero_()\n",
        "\n",
        "    # Disable gradient settings.\n",
        "    requires_grad = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        requires_grad[name] = param.requires_grad\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # Attach a hook to the model at the desired layer.\n",
        "    assert isinstance(layer, nn.Module)\n",
        "    with Hook(layer) as hook:\n",
        "        # Do a forward and backward pass.\n",
        "        output = model(input)\n",
        "        output.backward(target)\n",
        "\n",
        "        grad = hook.gradient.float()\n",
        "        act = hook.activation.float()\n",
        "\n",
        "        # Global average pool gradient across spatial dimension\n",
        "        # to obtain importance weights.\n",
        "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
        "        # Weighted combination of activation maps over channel\n",
        "        # dimension.\n",
        "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
        "        # We only want neurons with positive influence so we\n",
        "        # clamp any negative ones.\n",
        "        gradcam = torch.clamp(gradcam, min=0)\n",
        "\n",
        "    # Resize gradcam to input resolution.\n",
        "    gradcam = F.interpolate(\n",
        "        gradcam,\n",
        "        input.shape[2:],\n",
        "        mode='bicubic',\n",
        "        align_corners=False)\n",
        "\n",
        "    # Restore gradient settings.\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad_(requires_grad[name])\n",
        "\n",
        "    return gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228e95aa",
      "metadata": {
        "id": "228e95aa"
      },
      "outputs": [],
      "source": [
        "#@title Run\n",
        "\n",
        "#@markdown #### Image & Caption settings\n",
        "#image_url = '/content/image.png' #@param {type:\"string\"}\n",
        "\n",
        "image_caption = 'SHE is hot' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### CLIP model settings\n",
        "clip_model = \"RN50\" #@param [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\"]\n",
        "saliency_layer = \"layer4\" #@param [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
        "#@markdown ---\n",
        "#@markdown #### Visualization settings\n",
        "blur = True #@param {type:\"boolean\"}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(clip_model, device=device, jit=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb09e09",
      "metadata": {
        "id": "1fb09e09"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db6c2e3",
      "metadata": {
        "id": "2db6c2e3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb3e416",
      "metadata": {
        "id": "0cb3e416"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae22010",
      "metadata": {
        "id": "9ae22010"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beeff0c6",
      "metadata": {
        "id": "beeff0c6"
      },
      "outputs": [],
      "source": [
        "#THIS CODE WOrking properly for bluring using clip RN50 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d7a1fb",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ea8d34504ccf4767be608c4b4aca646f"
          ]
        },
        "id": "e2d7a1fb",
        "outputId": "2ce786de-b293-4319-f987-4335ddb9367f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
            "\n",
            "  | Name             | Type            | Params\n",
            "-----------------------------------------------------\n",
            "0 | MFB              | MFB             | 16.8 M\n",
            "1 | fin_y_shape      | Linear          | 393 K \n",
            "2 | clip_model       | CLIP            | 151 M \n",
            "3 | fin_old          | Linear          | 130   \n",
            "4 | fin              | Linear          | 786 K \n",
            "5 | image_classifier | ImageClassifier | 152 M \n",
            "-----------------------------------------------------\n",
            "321 M     Trainable params\n",
            "0         Non-trainable params\n",
            "321 M     Total params\n",
            "643.721   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea8d34504ccf4767be608c4b4aca646f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/DATA/atul_2221cs20/anaconda3/envs/jitu/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "#version05\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import CLIPProcessor\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import clip\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "\n",
        "# Load CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Define attention mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(feature_dim, feature_dim).to(device)\n",
        "        self.key_proj = nn.Linear(feature_dim, feature_dim).to(device)\n",
        "        self.value_proj = nn.Linear(feature_dim, feature_dim).to(device)\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        #print(' image_features', image_features.size)\n",
        "        feature_dim = 512\n",
        "\n",
        "        # Convert NumPy array to PyTorch tensor and move to device\n",
        "        image_features = torch.tensor(image_features).to(device).float()\n",
        "       # print(\"image_features shape (input):\", image_features.shape)\n",
        "\n",
        "\n",
        "        # Apply linear projections for Q, K, and V\n",
        "        Q = self.query_proj(image_features)\n",
        "        K = self.key_proj(image_features)\n",
        "        V = self.value_proj(image_features)\n",
        "\n",
        "        # Compute attention scores and weights\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / image_features.shape[-1]**0.5\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "\n",
        "        # Apply attention weights to obtain the weighted features\n",
        "        weighted_features = torch.matmul(attention_weights, V)\n",
        "\n",
        "\n",
        "        # Sum along the last dimension to get the final attention output\n",
        "        #output_features = torch.sum(weighted_features, dim=-2)\n",
        "        #print(' weighted_features', weighted_features)\n",
        "        #print(\"weighted_features shape (output):\", weighted_features.shape)\n",
        "\n",
        "        return weighted_features , attention_weights\n",
        "# Define PredictionModel\n",
        "class PredictionModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512).to(device)\n",
        "        self.text_embedding = nn.Linear(512, 512).to(device)  # Project text to 512 dimensions\n",
        "        self.output_layer = nn.Linear(512 + 512, num_classes).to(device)  # Combine 1024 features\n",
        "    def forward(self, masked_features, text_features=None):\n",
        "        masked_features = masked_features.float()\n",
        "        x = F.relu(self.fc1(masked_features))\n",
        "\n",
        "        if text_features is not None:\n",
        "            text_features = text_features.float()\n",
        "            text_emb = F.relu(self.text_embedding(text_features))\n",
        "           # print(\"x shape:\", x.shape)\n",
        "           # print(\"text_emb shape:\", text_emb.shape)\n",
        "            x = torch.cat([x, text_emb], dim=1)\n",
        "\n",
        "        prediction = self.output_layer(x)\n",
        "        #print('prediction',prediction)\n",
        "        return prediction\n",
        "\n",
        "# Define ImageClassifier\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.attention_layer = SelfAttention(feature_dim=512)\n",
        "        self.classifier = PredictionModel(input_dim=512, num_classes=2)\n",
        "\n",
        "    def forward(self, image_tensor, text_tensor):\n",
        "        #image_tensor = image_tensor.unsqueeze(1)\n",
        "       # image_tensor = image_tensor.repeat(1, 3, 1, 1)\n",
        "        #image_features = clip_model.encode_image(image_tensor)\n",
        "        #print('image_tensor',image_tensor.shape)\n",
        "        #print('text_tensor',text_tensor.shape)\n",
        "\n",
        "        masked_features, attention_weights = self.attention_layer(image_tensor)\n",
        "        #print('masked_features',masked_features)\n",
        "        #print('masked_features shape',masked_features.shape)\n",
        "        prediction = self.classifier(masked_features, text_tensor)\n",
        "        #print('classifier prediction', prediction )\n",
        "        return prediction, attention_weights\n",
        "def apply_attention(image, attention_weights, image_path,text_description):\n",
        "\n",
        "    original_image = cv2.imread(image_path)\n",
        "    # Download the image from the web.\n",
        "    image_caption = text_description #@param {type:\"string\"}\n",
        "    #@markdown ---\n",
        "    #@markdown #### CLIP model settings\n",
        "    clip_model = \"RN50\" #@param [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\"]\n",
        "    saliency_layer = \"layer4\" #@param [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
        "    #@markdown ---\n",
        "    #@markdown #### Visualization settings\n",
        "    blur = True #@param {type:\"boolean\"}\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(clip_model, device=device, jit=False)\n",
        "    image_path = image_path\n",
        "    #urllib.request.urlretrieve(image_url, image_path)\n",
        "\n",
        "    image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "    image_np = load_image(image_path, model.visual.input_resolution)\n",
        "    text_input = clip.tokenize([image_caption[:77]]).to(device)\n",
        "\n",
        "   # _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    attn_map = gradCAM(\n",
        "        model.visual,\n",
        "        image_input,\n",
        "        model.encode_text(text_input).float(),\n",
        "        getattr(model.visual, saliency_layer)\n",
        "    )\n",
        "    attn_map = attn_map.squeeze().detach().cpu().numpy()\n",
        "#     with np.printoptions(threshold=np.inf):\n",
        "#         print(attn_map)\n",
        "    ##print('attn_map',attn_map.shape)\n",
        "    binary_mask=(attn_map>0.07)\n",
        "   # print('binary_mask',binary_mask)\n",
        "\n",
        "#     plt.figure(figsize=[22,22])\n",
        "#     plt.subplot(121);plt.imshow(attn_map);plt.title(\"original Image\");plt.axis('off');\n",
        "\n",
        "    binary_mask_3=np.dstack((binary_mask,binary_mask,binary_mask))\n",
        "\n",
        "    #print('binary_mask',binary_mask_3.shape)\n",
        "   # print('attn_map',attn_map.shape)\n",
        "    blurred_image=cv2.GaussianBlur(image_np,(25, 25),0)\n",
        "    image= cv2.resize(image, (224,224))\n",
        "\n",
        "\n",
        "  #  print('blurred_image',blurred_image.shape)\n",
        "  #  print('image',image.shape)\n",
        "    blurred_image = (blurred_image * 255).astype('uint8') # Scale and convert to 'uint8'\n",
        "    blurred_image = cv2.cvtColor(blurred_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "#     axes[0].imshow(blurred_image)\n",
        "#     axes[1].imshow(image)\n",
        "#     for ax in axes:\n",
        "#         ax.axis(\"off\")\n",
        "#     plt.show()\n",
        "\n",
        "    o_i=np.where(binary_mask_3,blurred_image,image)\n",
        "    o_i= cv2.resize(o_i, (640,640))\n",
        "\n",
        "    #reversed_img = img.transpose(2, 0, 1)\n",
        "    #plt.imshow(o_i)\n",
        "#     _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "#     axes[0].imshow(o_i)\n",
        "#     axes[1].imshow(getAttMap(image_np, attn_map, blur))\n",
        "#     for ax in axes:\n",
        "#         ax.axis(\"off\")\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "   # viz_attn(image_np, attn_map, blur)\n",
        "\n",
        "    #modified_image = getAttMap(image_np, attn_map, blur)\n",
        "    #modified_image = (modified_image * 255).astype('uint8') # Scale and convert to 'uint8'\n",
        "    #modified_image = cv2.cvtColor(modified_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    #cv2.imwrite(\"Blurred1eeeee.jpg\",  o_i)\n",
        "\n",
        "    return o_i\n",
        "\n",
        "def process_image(image_path,attention_weights,t,text_description):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "    # Optional: You can use the attention weights for visualization\n",
        "    attention_heatmap = cv2.resize(attention_weights[0].detach().cpu().numpy(), (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # Apply blur only to misogynistic regions\n",
        "    modified_image = apply_attention(image, attention_weights,image_path,text_description)\n",
        "\n",
        "    # Save the modified image and attention heatmap\n",
        "    filename, ext = os.path.splitext(os.path.basename(image_path))\n",
        "    modified_filename = filename + \"_modified\" + ext\n",
        "    attention_heatmap_filename = filename + \"_attention_heatmap\" + ext\n",
        "\n",
        "    output_dir = \"output_images_RN50\"+str(t)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    modified_path = os.path.join(output_dir, modified_filename)\n",
        "    heatmap_path = os.path.join(output_dir, attention_heatmap_filename)\n",
        "\n",
        "    cv2.imwrite(modified_path, modified_image)\n",
        "   # cv2.imwrite(heatmap_path, (attention_heatmap * 255).astype(np.uint8))\n",
        "\n",
        "    return modified_image, attention_heatmap\n",
        "# Define Classifier class\n",
        "class Classifier(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.MFB = MFB(512, 512, True, 256, 64, 0.1)\n",
        "        self.fin_y_shape = torch.nn.Linear(768, 512)\n",
        "        self.clip_model, self.compose = clip.load(\"ViT-B/32\", device=device)\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "        self.fin_old = torch.nn.Linear(64, 2)  # producing final output logits\n",
        "        self.fin = torch.nn.Linear(16 * 768, 64)  # linear layer to the fused features\n",
        "        self.image_classifier = ImageClassifier()\n",
        "\n",
        "    def mask_and_predict(self, text):\n",
        "        inputs = clip.tokenize(text, truncate=True).to(self.device)\n",
        "        text_features = self.clip_model.encode_text(inputs).detach().cpu().numpy()\n",
        "        text_features = torch.tensor(text_features).to(self.device)\n",
        "        text_features = text_features.view(text_features.size(0), -1)\n",
        "        return text_features\n",
        "\n",
        "    def forward(self, x, y, text):\n",
        "        x_, y_ = x, y\n",
        "        text_features = self.mask_and_predict(text)\n",
        "        z = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(text_features, axis=1))\n",
        "        z_new = torch.squeeze(z, dim=1)\n",
        "        c = self.fin_old(z_new)\n",
        "        output = torch.log_softmax(c, dim=1)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        lab, txt, img, name, test_description = train_batch\n",
        "        lab = train_batch[lab]\n",
        "        txt = train_batch[txt]\n",
        "        img = train_batch[img]\n",
        "        name = train_batch[name]\n",
        "        test_description = train_batch[test_description]\n",
        "\n",
        "        logits, attention_weights = self.image_classifier(img, txt)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "        predicted_label = predicted_label.detach().cpu().numpy()\n",
        "        prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "        loss = F.cross_entropy(logits, lab)\n",
        "        self.log('train_loss', loss)\n",
        "        if self.current_epoch == (self.trainer.max_epochs - 1):\n",
        "           #print('attention_weights',attention_weights)\n",
        "           t= self.trainer.max_epochs - 1\n",
        "           for i in range(len(img)):\n",
        "               image_path = '/DATA/atul_2221cs20/jitendra/MAMI_2022_images/training_images/'+name[i]\n",
        "                # Assuming name contains the image paths\n",
        "               #print(image_path)\n",
        "               text_description=test_description[i]\n",
        "               process_image(image_path, attention_weights[i],t,text_description)\n",
        "        return loss\n",
        "\n",
        "#     def validation_step(self, val_batch, batch_idx):\n",
        "#         lab, txt, img, name, test_description = val_batch\n",
        "#         lab = val_batch[lab]\n",
        "#         txt = val_batch[txt]\n",
        "#         img = val_batch[img]\n",
        "#         test_description = val_batch[test_description]\n",
        "\n",
        "#         logits, attention_weights = self.image_classifier(img, txt)\n",
        "#         probabilities = torch.softmax(logits, dim=1)\n",
        "#         prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "#         predicted_label = predicted_label.detach().cpu().numpy()\n",
        "#         prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "#         loss = self.cross_entropy_loss(logits, lab)\n",
        "#         self.log('val_acc', accuracy_score(lab, predicted_label))\n",
        "#         self.log('val_roc_auc', roc_auc_score(lab, predicted_label))\n",
        "#         self.log('val_loss', loss)\n",
        "#         return {\n",
        "#             'progress_bar': {'val_acc': accuracy_score(lab, predicted_label)},\n",
        "#             'val_f1_offensive': f1_score(lab, predicted_label, average='macro')\n",
        "#         }\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        outs = [out['progress_bar']['val_acc'] for out in validation_step_outputs]\n",
        "        outs_f1 = [out['val_f1_offensive'] for out in validation_step_outputs]\n",
        "        self.log('val_acc_all_offn', sum(outs) / len(outs))\n",
        "        self.log('val_f1_offensive', sum(outs_f1) / len(outs_f1))\n",
        "        print(f'***val_acc_all_offn at epoch end {sum(outs) / len(outs)}****')\n",
        "        print(f'***val_f1_offensive at epoch end {sum(outs_f1) / len(outs_f1)}****')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        lab, txt, img, name, text_description = batch\n",
        "        lab = batch[lab]\n",
        "        name = batch[name]\n",
        "        text_description = batch[text_description]\n",
        "        txt = batch[txt]\n",
        "        img = batch[img]\n",
        "\n",
        "        logits, attention_weights = self.image_classifier(img, txt)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "        predicted_label = predicted_label.detach().cpu().numpy()\n",
        "        prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "        loss = F.cross_entropy(logits, lab)\n",
        "        lab = lab.detach().cpu().numpy()\n",
        "        self.log('test_acc', accuracy_score(lab, predicted_label))\n",
        "        self.log('test_roc_auc', roc_auc_score(lab, predicted_label))\n",
        "        self.log('test_loss', loss)\n",
        "        tqdm_dict = {'test_acc': accuracy_score(lab, predicted_label)}\n",
        "\n",
        "        return {\n",
        "            'progress_bar': tqdm_dict,\n",
        "            'test_acc': accuracy_score(lab, predicted_label),\n",
        "            'test_f1_score': f1_score(lab, predicted_label, average='macro'),\n",
        "        }\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        outs = [out['test_acc'] for out in outputs]\n",
        "        outs_f1 = [out['test_f1_score'] for out in outputs]\n",
        "        self.log('test_acc', sum(outs) / len(outs))\n",
        "        self.log('test_f1_score', sum(outs_f1) / len(outs_f1))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "    def setup(self, stage):\n",
        "        self.hm_train = t_p\n",
        "        self.hm_val = v_p\n",
        "        self.hm_test = te_p\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.hm_train, batch_size=128, drop_last=True)\n",
        "\n",
        "#     def val_dataloader(self):\n",
        "#         return DataLoader(self.hm_val, batch_size=128, drop_last=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.hm_test, batch_size=128, drop_last=True)\n",
        "\n",
        "# Data setup\n",
        "data_module = HmDataModule()\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_acc_all_offn',\n",
        "    dirpath='Jitendra/',\n",
        "    filename=\"epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}\",\n",
        "    auto_insert_metric_name=False,\n",
        "    save_top_k=1,\n",
        "    mode=\"max\",\n",
        ")\n",
        "all_callbacks = [checkpoint_callback]\n",
        "\n",
        "# Model training\n",
        "seed_everything(42, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus, deterministic=True, max_epochs=1, precision=16, callbacks=all_callbacks)\n",
        "trainer.fit(hm_model, data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d92923",
      "metadata": {
        "id": "22d92923"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852386f3",
      "metadata": {
        "id": "852386f3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659d96dd",
      "metadata": {
        "id": "659d96dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffe2b3d",
      "metadata": {
        "id": "1ffe2b3d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02cfe4d4",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "97b4932e995d43188212fc5cf3b453b1"
          ]
        },
        "id": "02cfe4d4",
        "outputId": "4bab372a-d6b3-4149-b9bb-3ac83015e730"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
            "\n",
            "  | Name             | Type            | Params\n",
            "-----------------------------------------------------\n",
            "0 | MFB              | MFB             | 16.8 M\n",
            "1 | fin_y_shape      | Linear          | 393 K \n",
            "2 | clip_model       | CLIP            | 151 M \n",
            "3 | fin_old          | Linear          | 130   \n",
            "4 | fin              | Linear          | 786 K \n",
            "5 | image_classifier | ImageClassifier | 152 M \n",
            "-----------------------------------------------------\n",
            "321 M     Trainable params\n",
            "0         Non-trainable params\n",
            "321 M     Total params\n",
            "643.721   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97b4932e995d43188212fc5cf3b453b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/DATA/atul_2221cs20/anaconda3/envs/jitu/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " weighted_features tensor([[ 0.0130,  0.2676, -0.1196,  ..., -0.1937, -0.0708,  0.0917],\n",
            "        [ 0.0131,  0.2683, -0.1176,  ..., -0.1920, -0.0693,  0.0922],\n",
            "        [ 0.0139,  0.2676, -0.1172,  ..., -0.1931, -0.0695,  0.0917],\n",
            "        ...,\n",
            "        [ 0.0142,  0.2668, -0.1169,  ..., -0.1930, -0.0675,  0.0919],\n",
            "        [ 0.0128,  0.2676, -0.1168,  ..., -0.1926, -0.0692,  0.0920],\n",
            "        [ 0.0128,  0.2676, -0.1188,  ..., -0.1924, -0.0695,  0.0918]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
            "weighted_features shape (output): torch.Size([128, 512])\n",
            "masked_features tensor([[ 0.0130,  0.2676, -0.1196,  ..., -0.1937, -0.0708,  0.0917],\n",
            "        [ 0.0131,  0.2683, -0.1176,  ..., -0.1920, -0.0693,  0.0922],\n",
            "        [ 0.0139,  0.2676, -0.1172,  ..., -0.1931, -0.0695,  0.0917],\n",
            "        ...,\n",
            "        [ 0.0142,  0.2668, -0.1169,  ..., -0.1930, -0.0675,  0.0919],\n",
            "        [ 0.0128,  0.2676, -0.1168,  ..., -0.1926, -0.0692,  0.0920],\n",
            "        [ 0.0128,  0.2676, -0.1188,  ..., -0.1924, -0.0695,  0.0918]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
            "masked_features shape torch.Size([128, 512])\n",
            "prediction tensor([[ 0.1045,  0.0404],\n",
            "        [ 0.1373,  0.0511],\n",
            "        [ 0.1157,  0.0524],\n",
            "        [ 0.1267,  0.1116],\n",
            "        [ 0.1353,  0.0662],\n",
            "        [ 0.0809,  0.0603],\n",
            "        [ 0.0906,  0.1052],\n",
            "        [ 0.1317,  0.1290],\n",
            "        [ 0.0330,  0.0778],\n",
            "        [ 0.0584,  0.0502],\n",
            "        [ 0.0803,  0.1190],\n",
            "        [ 0.0984,  0.0385],\n",
            "        [ 0.1293,  0.1135],\n",
            "        [ 0.1008,  0.0231],\n",
            "        [ 0.0963,  0.1290],\n",
            "        [ 0.1095,  0.0840],\n",
            "        [ 0.1608,  0.0742],\n",
            "        [ 0.1346,  0.0693],\n",
            "        [ 0.0975,  0.1052],\n",
            "        [ 0.2052,  0.1560],\n",
            "        [ 0.0992,  0.0165],\n",
            "        [ 0.0586,  0.0789],\n",
            "        [ 0.1299,  0.0927],\n",
            "        [ 0.1575,  0.0785],\n",
            "        [ 0.0464,  0.0320],\n",
            "        [ 0.0889,  0.0096],\n",
            "        [ 0.1392,  0.0442],\n",
            "        [ 0.0575,  0.0866],\n",
            "        [ 0.1388,  0.0701],\n",
            "        [ 0.0944,  0.0835],\n",
            "        [ 0.0963,  0.0120],\n",
            "        [ 0.0539, -0.0295],\n",
            "        [ 0.1792,  0.0206],\n",
            "        [ 0.0675,  0.0469],\n",
            "        [ 0.1151,  0.0969],\n",
            "        [ 0.0767,  0.0836],\n",
            "        [ 0.1169,  0.1040],\n",
            "        [ 0.1069,  0.0797],\n",
            "        [ 0.0758,  0.0559],\n",
            "        [ 0.1212,  0.1074],\n",
            "        [ 0.0224,  0.1051],\n",
            "        [ 0.1218,  0.0994],\n",
            "        [ 0.0742,  0.0606],\n",
            "        [ 0.1013,  0.0922],\n",
            "        [ 0.0723, -0.0129],\n",
            "        [ 0.1073,  0.0121],\n",
            "        [ 0.1216,  0.0955],\n",
            "        [ 0.0850,  0.0946],\n",
            "        [ 0.0897,  0.0845],\n",
            "        [ 0.1394,  0.0646],\n",
            "        [ 0.0992,  0.0325],\n",
            "        [ 0.0379,  0.0557],\n",
            "        [ 0.0944,  0.1051],\n",
            "        [ 0.0531,  0.1541],\n",
            "        [ 0.1665,  0.0469],\n",
            "        [ 0.1382, -0.0049],\n",
            "        [ 0.1415,  0.0180],\n",
            "        [ 0.1322,  0.0883],\n",
            "        [ 0.1046,  0.1372],\n",
            "        [ 0.1069,  0.0834],\n",
            "        [ 0.1316,  0.0319],\n",
            "        [ 0.0461,  0.1133],\n",
            "        [ 0.1307,  0.0682],\n",
            "        [ 0.1051,  0.0332],\n",
            "        [ 0.1302,  0.0081],\n",
            "        [ 0.0562,  0.1443],\n",
            "        [ 0.1123,  0.1064],\n",
            "        [ 0.1462,  0.1420],\n",
            "        [ 0.0692, -0.0212],\n",
            "        [ 0.1071,  0.1021],\n",
            "        [ 0.1250,  0.1310],\n",
            "        [ 0.0407,  0.0450],\n",
            "        [ 0.0598,  0.0608],\n",
            "        [ 0.1693,  0.0696],\n",
            "        [ 0.0902, -0.0159],\n",
            "        [ 0.0978,  0.1228],\n",
            "        [ 0.1434,  0.0682],\n",
            "        [ 0.0723,  0.0158],\n",
            "        [ 0.0883,  0.0244],\n",
            "        [ 0.0945,  0.0992],\n",
            "        [ 0.1165,  0.0636],\n",
            "        [ 0.0772,  0.1289],\n",
            "        [ 0.1198,  0.0186],\n",
            "        [ 0.1273,  0.0487],\n",
            "        [ 0.0991,  0.0090],\n",
            "        [ 0.0916, -0.0036],\n",
            "        [ 0.0808,  0.1089],\n",
            "        [ 0.1254,  0.0131],\n",
            "        [ 0.1274,  0.0604],\n",
            "        [ 0.1013,  0.1017],\n",
            "        [ 0.0815,  0.0477],\n",
            "        [ 0.0952,  0.1141],\n",
            "        [ 0.0637,  0.1553],\n",
            "        [ 0.0959,  0.0439],\n",
            "        [ 0.1002,  0.0461],\n",
            "        [ 0.0413,  0.0513],\n",
            "        [ 0.0274,  0.0646],\n",
            "        [ 0.0324,  0.1166],\n",
            "        [ 0.0186,  0.0391],\n",
            "        [ 0.0117,  0.0186],\n",
            "        [ 0.0845,  0.1256],\n",
            "        [ 0.0481,  0.0237],\n",
            "        [ 0.0816,  0.0457],\n",
            "        [ 0.1404,  0.0479],\n",
            "        [ 0.1752,  0.0534],\n",
            "        [ 0.0866,  0.0698],\n",
            "        [ 0.1760,  0.0814],\n",
            "        [ 0.1095,  0.1482],\n",
            "        [ 0.1378,  0.1213],\n",
            "        [ 0.0981,  0.1158],\n",
            "        [ 0.1151,  0.0809],\n",
            "        [ 0.1055,  0.0933],\n",
            "        [ 0.1119,  0.0764],\n",
            "        [ 0.1232,  0.0495],\n",
            "        [ 0.1666,  0.0753],\n",
            "        [ 0.0963,  0.0957],\n",
            "        [ 0.0286,  0.0424],\n",
            "        [ 0.1022, -0.0054],\n",
            "        [ 0.1292,  0.0811],\n",
            "        [ 0.0598,  0.0231],\n",
            "        [ 0.0601,  0.1252],\n",
            "        [ 0.1361,  0.1080],\n",
            "        [ 0.1470,  0.0182],\n",
            "        [ 0.0934,  0.0457],\n",
            "        [ 0.1044,  0.0520],\n",
            "        [ 0.1812,  0.0785],\n",
            "        [ 0.0889,  0.0526],\n",
            "        [ 0.0552,  0.1136]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "classifier prediction tensor([[ 0.1045,  0.0404],\n",
            "        [ 0.1373,  0.0511],\n",
            "        [ 0.1157,  0.0524],\n",
            "        [ 0.1267,  0.1116],\n",
            "        [ 0.1353,  0.0662],\n",
            "        [ 0.0809,  0.0603],\n",
            "        [ 0.0906,  0.1052],\n",
            "        [ 0.1317,  0.1290],\n",
            "        [ 0.0330,  0.0778],\n",
            "        [ 0.0584,  0.0502],\n",
            "        [ 0.0803,  0.1190],\n",
            "        [ 0.0984,  0.0385],\n",
            "        [ 0.1293,  0.1135],\n",
            "        [ 0.1008,  0.0231],\n",
            "        [ 0.0963,  0.1290],\n",
            "        [ 0.1095,  0.0840],\n",
            "        [ 0.1608,  0.0742],\n",
            "        [ 0.1346,  0.0693],\n",
            "        [ 0.0975,  0.1052],\n",
            "        [ 0.2052,  0.1560],\n",
            "        [ 0.0992,  0.0165],\n",
            "        [ 0.0586,  0.0789],\n",
            "        [ 0.1299,  0.0927],\n",
            "        [ 0.1575,  0.0785],\n",
            "        [ 0.0464,  0.0320],\n",
            "        [ 0.0889,  0.0096],\n",
            "        [ 0.1392,  0.0442],\n",
            "        [ 0.0575,  0.0866],\n",
            "        [ 0.1388,  0.0701],\n",
            "        [ 0.0944,  0.0835],\n",
            "        [ 0.0963,  0.0120],\n",
            "        [ 0.0539, -0.0295],\n",
            "        [ 0.1792,  0.0206],\n",
            "        [ 0.0675,  0.0469],\n",
            "        [ 0.1151,  0.0969],\n",
            "        [ 0.0767,  0.0836],\n",
            "        [ 0.1169,  0.1040],\n",
            "        [ 0.1069,  0.0797],\n",
            "        [ 0.0758,  0.0559],\n",
            "        [ 0.1212,  0.1074],\n",
            "        [ 0.0224,  0.1051],\n",
            "        [ 0.1218,  0.0994],\n",
            "        [ 0.0742,  0.0606],\n",
            "        [ 0.1013,  0.0922],\n",
            "        [ 0.0723, -0.0129],\n",
            "        [ 0.1073,  0.0121],\n",
            "        [ 0.1216,  0.0955],\n",
            "        [ 0.0850,  0.0946],\n",
            "        [ 0.0897,  0.0845],\n",
            "        [ 0.1394,  0.0646],\n",
            "        [ 0.0992,  0.0325],\n",
            "        [ 0.0379,  0.0557],\n",
            "        [ 0.0944,  0.1051],\n",
            "        [ 0.0531,  0.1541],\n",
            "        [ 0.1665,  0.0469],\n",
            "        [ 0.1382, -0.0049],\n",
            "        [ 0.1415,  0.0180],\n",
            "        [ 0.1322,  0.0883],\n",
            "        [ 0.1046,  0.1372],\n",
            "        [ 0.1069,  0.0834],\n",
            "        [ 0.1316,  0.0319],\n",
            "        [ 0.0461,  0.1133],\n",
            "        [ 0.1307,  0.0682],\n",
            "        [ 0.1051,  0.0332],\n",
            "        [ 0.1302,  0.0081],\n",
            "        [ 0.0562,  0.1443],\n",
            "        [ 0.1123,  0.1064],\n",
            "        [ 0.1462,  0.1420],\n",
            "        [ 0.0692, -0.0212],\n",
            "        [ 0.1071,  0.1021],\n",
            "        [ 0.1250,  0.1310],\n",
            "        [ 0.0407,  0.0450],\n",
            "        [ 0.0598,  0.0608],\n",
            "        [ 0.1693,  0.0696],\n",
            "        [ 0.0902, -0.0159],\n",
            "        [ 0.0978,  0.1228],\n",
            "        [ 0.1434,  0.0682],\n",
            "        [ 0.0723,  0.0158],\n",
            "        [ 0.0883,  0.0244],\n",
            "        [ 0.0945,  0.0992],\n",
            "        [ 0.1165,  0.0636],\n",
            "        [ 0.0772,  0.1289],\n",
            "        [ 0.1198,  0.0186],\n",
            "        [ 0.1273,  0.0487],\n",
            "        [ 0.0991,  0.0090],\n",
            "        [ 0.0916, -0.0036],\n",
            "        [ 0.0808,  0.1089],\n",
            "        [ 0.1254,  0.0131],\n",
            "        [ 0.1274,  0.0604],\n",
            "        [ 0.1013,  0.1017],\n",
            "        [ 0.0815,  0.0477],\n",
            "        [ 0.0952,  0.1141],\n",
            "        [ 0.0637,  0.1553],\n",
            "        [ 0.0959,  0.0439],\n",
            "        [ 0.1002,  0.0461],\n",
            "        [ 0.0413,  0.0513],\n",
            "        [ 0.0274,  0.0646],\n",
            "        [ 0.0324,  0.1166],\n",
            "        [ 0.0186,  0.0391],\n",
            "        [ 0.0117,  0.0186],\n",
            "        [ 0.0845,  0.1256],\n",
            "        [ 0.0481,  0.0237],\n",
            "        [ 0.0816,  0.0457],\n",
            "        [ 0.1404,  0.0479],\n",
            "        [ 0.1752,  0.0534],\n",
            "        [ 0.0866,  0.0698],\n",
            "        [ 0.1760,  0.0814],\n",
            "        [ 0.1095,  0.1482],\n",
            "        [ 0.1378,  0.1213],\n",
            "        [ 0.0981,  0.1158],\n",
            "        [ 0.1151,  0.0809],\n",
            "        [ 0.1055,  0.0933],\n",
            "        [ 0.1119,  0.0764],\n",
            "        [ 0.1232,  0.0495],\n",
            "        [ 0.1666,  0.0753],\n",
            "        [ 0.0963,  0.0957],\n",
            "        [ 0.0286,  0.0424],\n",
            "        [ 0.1022, -0.0054],\n",
            "        [ 0.1292,  0.0811],\n",
            "        [ 0.0598,  0.0231],\n",
            "        [ 0.0601,  0.1252],\n",
            "        [ 0.1361,  0.1080],\n",
            "        [ 0.1470,  0.0182],\n",
            "        [ 0.0934,  0.0457],\n",
            "        [ 0.1044,  0.0520],\n",
            "        [ 0.1812,  0.0785],\n",
            "        [ 0.0889,  0.0526],\n",
            "        [ 0.0552,  0.1136]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "attention_weights tensor([[0.0080, 0.0079, 0.0080,  ..., 0.0072, 0.0082, 0.0074],\n",
            "        [0.0078, 0.0077, 0.0075,  ..., 0.0070, 0.0086, 0.0079],\n",
            "        [0.0078, 0.0078, 0.0078,  ..., 0.0081, 0.0085, 0.0075],\n",
            "        ...,\n",
            "        [0.0080, 0.0077, 0.0076,  ..., 0.0073, 0.0082, 0.0082],\n",
            "        [0.0078, 0.0077, 0.0073,  ..., 0.0076, 0.0082, 0.0080],\n",
            "        [0.0082, 0.0076, 0.0079,  ..., 0.0074, 0.0081, 0.0076]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#version03\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import CLIPProcessor\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import clip\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "\n",
        "# Load CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Define attention mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(feature_dim, feature_dim).to(device)\n",
        "        self.key_proj = nn.Linear(feature_dim, feature_dim).to(device)\n",
        "        self.value_proj = nn.Linear(feature_dim, feature_dim).to(device)\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        #print(' image_features', image_features.size)\n",
        "        feature_dim = 512\n",
        "\n",
        "        # Convert NumPy array to PyTorch tensor and move to device\n",
        "        image_features = torch.tensor(image_features).to(device).float()\n",
        "       # print(\"image_features shape (input):\", image_features.shape)\n",
        "\n",
        "\n",
        "        # Apply linear projections for Q, K, and V\n",
        "        Q = self.query_proj(image_features)\n",
        "        K = self.key_proj(image_features)\n",
        "        V = self.value_proj(image_features)\n",
        "\n",
        "        # Compute attention scores and weights\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / image_features.shape[-1]**0.5\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "\n",
        "        # Apply attention weights to obtain the weighted features\n",
        "        weighted_features = torch.matmul(attention_weights, V)\n",
        "\n",
        "\n",
        "        # Sum along the last dimension to get the final attention output\n",
        "        #output_features = torch.sum(weighted_features, dim=-2)\n",
        "        print(' weighted_features', weighted_features)\n",
        "        print(\"weighted_features shape (output):\", weighted_features.shape)\n",
        "\n",
        "        return weighted_features , attention_weights\n",
        "# Define PredictionModel\n",
        "class PredictionModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512).to(device)\n",
        "        self.text_embedding = nn.Linear(512, 512).to(device)  # Project text to 512 dimensions\n",
        "        self.output_layer = nn.Linear(512 + 512, num_classes).to(device)  # Combine 1024 features\n",
        "    def forward(self, masked_features, text_features=None):\n",
        "        masked_features = masked_features.float()\n",
        "        x = F.relu(self.fc1(masked_features))\n",
        "\n",
        "        if text_features is not None:\n",
        "            text_features = text_features.float()\n",
        "            text_emb = F.relu(self.text_embedding(text_features))\n",
        "           # print(\"x shape:\", x.shape)\n",
        "           # print(\"text_emb shape:\", text_emb.shape)\n",
        "            x = torch.cat([x, text_emb], dim=1)\n",
        "\n",
        "        prediction = self.output_layer(x)\n",
        "        print('prediction',prediction)\n",
        "        return prediction\n",
        "\n",
        "# Define ImageClassifier\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.attention_layer = SelfAttention(feature_dim=512)\n",
        "        self.classifier = PredictionModel(input_dim=512, num_classes=2)\n",
        "\n",
        "    def forward(self, image_tensor, text_tensor):\n",
        "        #image_tensor = image_tensor.unsqueeze(1)\n",
        "       # image_tensor = image_tensor.repeat(1, 3, 1, 1)\n",
        "        #image_features = clip_model.encode_image(image_tensor)\n",
        "        #print('image_tensor',image_tensor.shape)\n",
        "        #print('text_tensor',text_tensor.shape)\n",
        "        masked_features, attention_weights = self.attention_layer(image_tensor)\n",
        "        print('masked_features',masked_features)\n",
        "        print('masked_features shape',masked_features.shape)\n",
        "        prediction = self.classifier(masked_features, text_tensor)\n",
        "        print('classifier prediction', prediction )\n",
        "        return prediction, attention_weights\n",
        "\n",
        "def apply_blur(image, attention_weights, image_path):\n",
        "    original_image = cv2.imread(image_path)\n",
        "    attention_weights_np = attention_weights.detach().cpu().numpy()\n",
        "    threshold = 0.008\n",
        "    blur_radius = 15\n",
        "\n",
        "    # Create mask based on attention weights (thresholding)\n",
        "    blur_mask = attention_weights_np <= threshold  # Corrected the condition\n",
        "\n",
        "    # Convert blur_mask to uint8 for resizing\n",
        "    blur_mask = blur_mask.astype('uint8')\n",
        "\n",
        "    # Resize\n",
        "    blur_mask = cv2.resize(blur_mask, (original_image.shape[1], original_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Apply blur\n",
        "    blurred_image = cv2.GaussianBlur(original_image.copy(), (blur_radius, blur_radius), 0, borderType=cv2.BORDER_CONSTANT)\n",
        "\n",
        "    # Ensure the shapes are consistent\n",
        "    blurred_image = blurred_image[..., ::-1]  # Reverse the order of channels (BGR to RGB)\n",
        "\n",
        "    # Apply the mask pixel by pixel\n",
        "    modified_image = np.copy(original_image)\n",
        "    for i in range(original_image.shape[0]):\n",
        "        for j in range(original_image.shape[1]):\n",
        "            if blur_mask[i, j] > 0:\n",
        "                modified_image[i, j] = blurred_image[i, j]\n",
        "\n",
        "    return modified_image\n",
        "def process_image(image_path,attention_weights,t):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "    # Optional: You can use the attention weights for visualization\n",
        "    attention_heatmap = cv2.resize(attention_weights[0].detach().cpu().numpy(), (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # Apply blur only to misogynistic regions\n",
        "    modified_image = apply_blur(image, attention_weights,image_path)\n",
        "\n",
        "    # Save the modified image and attention heatmap\n",
        "    filename, ext = os.path.splitext(os.path.basename(image_path))\n",
        "    modified_filename = filename + \"_modified\" + ext\n",
        "    attention_heatmap_filename = filename + \"_attention_heatmap\" + ext\n",
        "\n",
        "    output_dir = \"output_images1\"+str(t)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    modified_path = os.path.join(output_dir, modified_filename)\n",
        "    heatmap_path = os.path.join(output_dir, attention_heatmap_filename)\n",
        "\n",
        "    cv2.imwrite(modified_path, modified_image)\n",
        "    cv2.imwrite(heatmap_path, (attention_heatmap * 255).astype(np.uint8))\n",
        "\n",
        "    return modified_image, attention_heatmap\n",
        "# Define Classifier class\n",
        "class Classifier(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.MFB = MFB(512, 512, True, 256, 64, 0.1)\n",
        "        self.fin_y_shape = torch.nn.Linear(768, 512)\n",
        "        self.clip_model, self.compose = clip.load(\"ViT-B/32\", device=device)\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "        self.fin_old = torch.nn.Linear(64, 2)  # producing final output logits\n",
        "        self.fin = torch.nn.Linear(16 * 768, 64)  # linear layer to the fused features\n",
        "        self.image_classifier = ImageClassifier()\n",
        "\n",
        "    def mask_and_predict(self, text):\n",
        "        inputs = clip.tokenize(text, truncate=True).to(self.device)\n",
        "        text_features = self.clip_model.encode_text(inputs).detach().cpu().numpy()\n",
        "        text_features = torch.tensor(text_features).to(self.device)\n",
        "        text_features = text_features.view(text_features.size(0), -1)\n",
        "        return text_features\n",
        "\n",
        "    def forward(self, x, y, text):\n",
        "        x_, y_ = x, y\n",
        "        text_features = self.mask_and_predict(text)\n",
        "        z = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(text_features, axis=1))\n",
        "        z_new = torch.squeeze(z, dim=1)\n",
        "        c = self.fin_old(z_new)\n",
        "        output = torch.log_softmax(c, dim=1)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        lab, txt, img, name, test_description = train_batch\n",
        "        lab = train_batch[lab]\n",
        "        txt = train_batch[txt]\n",
        "        img = train_batch[img]\n",
        "        name = train_batch[name]\n",
        "        test_description = train_batch[test_description]\n",
        "\n",
        "        logits, attention_weights = self.image_classifier(img, txt)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "        predicted_label = predicted_label.detach().cpu().numpy()\n",
        "        prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "        loss = F.cross_entropy(logits, lab)\n",
        "        self.log('train_loss', loss)\n",
        "        if self.current_epoch == (self.trainer.max_epochs - 1):\n",
        "           print('attention_weights',attention_weights)\n",
        "           t= self.trainer.max_epochs - 1\n",
        "           for i in range(len(img)):\n",
        "               image_path = '/DATA/atul_2221cs20/jitendra/MAMI_2022_images/training_images/'+name[i]\n",
        "                # Assuming name contains the image paths\n",
        "               #print(image_path)\n",
        "               process_image(image_path, attention_weights[i],t)\n",
        "        return loss\n",
        "\n",
        "#     def validation_step(self, val_batch, batch_idx):\n",
        "#         lab, txt, img, name, test_description = val_batch\n",
        "#         lab = val_batch[lab]\n",
        "#         txt = val_batch[txt]\n",
        "#         img = val_batch[img]\n",
        "#         test_description = val_batch[test_description]\n",
        "\n",
        "#         logits, attention_weights = self.image_classifier(img, txt)\n",
        "#         probabilities = torch.softmax(logits, dim=1)\n",
        "#         prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "#         predicted_label = predicted_label.detach().cpu().numpy()\n",
        "#         prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "#         loss = self.cross_entropy_loss(logits, lab)\n",
        "#         self.log('val_acc', accuracy_score(lab, predicted_label))\n",
        "#         self.log('val_roc_auc', roc_auc_score(lab, predicted_label))\n",
        "#         self.log('val_loss', loss)\n",
        "#         return {\n",
        "#             'progress_bar': {'val_acc': accuracy_score(lab, predicted_label)},\n",
        "#             'val_f1_offensive': f1_score(lab, predicted_label, average='macro')\n",
        "#         }\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        outs = [out['progress_bar']['val_acc'] for out in validation_step_outputs]\n",
        "        outs_f1 = [out['val_f1_offensive'] for out in validation_step_outputs]\n",
        "        self.log('val_acc_all_offn', sum(outs) / len(outs))\n",
        "        self.log('val_f1_offensive', sum(outs_f1) / len(outs_f1))\n",
        "        print(f'***val_acc_all_offn at epoch end {sum(outs) / len(outs)}****')\n",
        "        print(f'***val_f1_offensive at epoch end {sum(outs_f1) / len(outs_f1)}****')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        lab, txt, img, name, text_description = batch\n",
        "        lab = batch[lab]\n",
        "        name = batch[name]\n",
        "        text_description = batch[text_description]\n",
        "        txt = batch[txt]\n",
        "        img = batch[img]\n",
        "\n",
        "        logits, attention_weights = self.image_classifier(img, txt)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "        predicted_label = predicted_label.detach().cpu().numpy()\n",
        "        prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "        loss = F.cross_entropy(logits, lab)\n",
        "        lab = lab.detach().cpu().numpy()\n",
        "        self.log('test_acc', accuracy_score(lab, predicted_label))\n",
        "        self.log('test_roc_auc', roc_auc_score(lab, predicted_label))\n",
        "        self.log('test_loss', loss)\n",
        "        tqdm_dict = {'test_acc': accuracy_score(lab, predicted_label)}\n",
        "\n",
        "        return {\n",
        "            'progress_bar': tqdm_dict,\n",
        "            'test_acc': accuracy_score(lab, predicted_label),\n",
        "            'test_f1_score': f1_score(lab, predicted_label, average='macro'),\n",
        "        }\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        outs = [out['test_acc'] for out in outputs]\n",
        "        outs_f1 = [out['test_f1_score'] for out in outputs]\n",
        "        self.log('test_acc', sum(outs) / len(outs))\n",
        "        self.log('test_f1_score', sum(outs_f1) / len(outs_f1))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "    def setup(self, stage):\n",
        "        self.hm_train = t_p\n",
        "        self.hm_val = v_p\n",
        "        self.hm_test = te_p\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.hm_train, batch_size=128, drop_last=True)\n",
        "\n",
        "#     def val_dataloader(self):\n",
        "#         return DataLoader(self.hm_val, batch_size=128, drop_last=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.hm_test, batch_size=128, drop_last=True)\n",
        "\n",
        "# Data setup\n",
        "data_module = HmDataModule()\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_acc_all_offn',\n",
        "    dirpath='Jitendra/',\n",
        "    filename=\"epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}\",\n",
        "    auto_insert_metric_name=False,\n",
        "    save_top_k=1,\n",
        "    mode=\"max\",\n",
        ")\n",
        "all_callbacks = [checkpoint_callback]\n",
        "\n",
        "# Model training\n",
        "seed_everything(42, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus, deterministic=True, max_epochs=1, precision=16, callbacks=all_callbacks)\n",
        "trainer.fit(hm_model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfa4685",
      "metadata": {
        "id": "bbfa4685"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d324ab7",
      "metadata": {
        "id": "4d324ab7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a79a1dd3",
      "metadata": {
        "id": "a79a1dd3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5801f0b9",
      "metadata": {
        "id": "5801f0b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "09bc1471",
      "metadata": {
        "id": "09bc1471"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e367200",
      "metadata": {
        "id": "5e367200"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331a0a56",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "",
            "d07556e6ea28417bb2fc3056c0ec349f"
          ]
        },
        "id": "331a0a56",
        "outputId": "980e55b0-f9b5-4e10-f003-d6c8c26762fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "/DATA/atul_2221cs20/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/DATA/atul_2221cs20/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /DATA/atul_2221cs20/jitendra/Jitendra exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
            "\n",
            "  | Name        | Type   | Params\n",
            "---------------------------------------\n",
            "0 | MFB         | MFB    | 16.8 M\n",
            "1 | fin_y_shape | Linear | 393 K \n",
            "2 | clip_model  | CLIP   | 151 M \n",
            "3 | fin_old     | Linear | 130   \n",
            "4 | fin         | Linear | 786 K \n",
            "---------------------------------------\n",
            "169 M     Trainable params\n",
            "0         Non-trainable params\n",
            "169 M     Total params\n",
            "338.535   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/DATA/atul_2221cs20/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***val_acc_all_offn at epoch end 0.54296875****\n",
            "***val_f1 offensive at epoch end 0.38690682942646515****\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/DATA/atul_2221cs20/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d07556e6ea28417bb2fc3056c0ec349f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#just for testing3\n",
        "\n",
        "class Classifier(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.MFB = MFB(512, 512, True, 256, 64, 0.1)\n",
        "    self.fin_y_shape = torch.nn.Linear(768, 512)\n",
        "    self.clip_model, self.compose = clip.load(\"ViT-B/32\", device=device)\n",
        "    self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    self.fin_old = torch.nn.Linear(64, 2)  # producing final output logits\n",
        "    self.fin = torch.nn.Linear(16 * 768, 64)  # linear layer to the fused features\n",
        "\n",
        "  def mask_and_predict(self, text):\n",
        "    #print(\"inside mask text\",text)\n",
        "    inputs = clip.tokenize(text, truncate=True).to(self.device)\n",
        "   # print(\"inside mask inputs\",inputs)\n",
        "\n",
        "    #for i in range(1, len(inputs[0]) - 1):\n",
        "    # masked_input_ids = inputs.clone()\n",
        "    # masked_input_ids[0][i] = 0  # Assuming 0 is the index for the [MASK] token\n",
        "    text_features = self.clip_model.encode_text(inputs).detach().cpu().numpy()\n",
        "    #print(\"inside the mask text_features1\",text_features,text_features.shape)\n",
        "    text_features = torch.tensor(text_features).to(self.device)\n",
        "    #print(\"inside the mask text_features2\",text_features,text_features.shape)\n",
        "    text_features = text_features.view(text_features.size(0), -1)\n",
        "\n",
        "    return text_features\n",
        "\n",
        "  def forward(self, x, y, text):\n",
        "\n",
        "    x_, y_ = x, y\n",
        "    text_features = self.mask_and_predict(text)\n",
        "    z = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(text_features, axis=1))\n",
        "    z_new = torch.squeeze(z, dim=1)\n",
        "    c = self.fin_old(z_new)\n",
        "    output = torch.log_softmax(c, dim=1)\n",
        "    return output\n",
        "  def forward1(self, x, y):\n",
        "\n",
        "    x_, y_ = x, y\n",
        "    #text_features = self.mask_and_predict(text)\n",
        "    z = self.MFB(torch.unsqueeze(y, axis=1), torch.unsqueeze(x, axis=1))\n",
        "    z_new = torch.squeeze(z, dim=1)\n",
        "    c = self.fin_old(z_new)\n",
        "    output = torch.log_softmax(c, dim=1)\n",
        "    return output\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "\n",
        "    return F.nll_loss(logits, labels)\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "\n",
        "    lab, txt, img, name, test_description = train_batch\n",
        "    lab = train_batch[lab]\n",
        "    txt = train_batch[txt]\n",
        "    img = train_batch[img]\n",
        "    name = train_batch[name]\n",
        "    test_description = train_batch[test_description]\n",
        "    logits = self.forward1(txt, img)\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    #print( probabilities)\n",
        "    prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "    predicted_label = predicted_label.detach().cpu().numpy()\n",
        "    prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "    csv_file_path = f\"hateful_words_results_epoch_{self.current_epoch + 1}.csv\"\n",
        "    with open(csv_file_path, mode='a', newline='') as file:\n",
        "        fieldnames = [\"image_name\", \"actual_label\", \"predicted_label\", \"prediction_probability\",\n",
        "                      \"top_hateful_word_1\", \"confidence_1\", \"top_hateful_word_2\", \"confidence_2\",\n",
        "                      \"top_hateful_word_3\", \"confidence_3\",\n",
        "                      \"top_hateful_word_4\", \"confidence_4\",\n",
        "                      \"top_hateful_word_5\", \"confidence_5\",\n",
        "                      \"top_hateful_word_6\", \"confidence_6\",\n",
        "                      \"top_hateful_word_7\", \"confidence_7\",\n",
        "                      \"top_hateful_word_8\", \"confidence_8\"\n",
        "                     ]\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "        # Write header only if the file is empty\n",
        "        if file.tell() == 0:\n",
        "            writer.writeheader()\n",
        "\n",
        "        for img_name, text_description, label,predicted_label,prediction_probability in zip(name, test_description, lab,predicted_label,prediction_probability):\n",
        "            predictions = []\n",
        "            word_impact = []\n",
        "            predicted_label=predicted_label.item()\n",
        "            prediction_probability=prediction_probability.item()\n",
        "            text_description_words = text_description.split()\n",
        "\n",
        "            # Iterate over each text description in the list\n",
        "            for description in text_description_words:\n",
        "                masked_text = ' '.join([word for word in text_description_words if word != description])\n",
        "                word_input = description\n",
        "                word_logits = self.forward(txt, img, masked_text)\n",
        "               # print(\"word_logits\",word_logits)\n",
        "                word_prediction = F.softmax(word_logits, dim=1)\n",
        "                #print(\"word_prediction\",word_prediction)\n",
        "                hate_confidence = word_prediction[0][1].item()\n",
        "                #print(\"hate_confidence\",hate_confidence)\n",
        "                word_impact.append((description, hate_confidence))\n",
        "\n",
        "            # Sort the words based on hate confidence in descending order\n",
        "            sorted_word_impact = sorted(word_impact, key=lambda x: x[1], )\n",
        "\n",
        "            # Take the top 3 most hateful words if available, otherwise fill with placeholders\n",
        "            top_hateful_words = sorted_word_impact[:8] if sorted_word_impact else [(\"\", 0)] * 8\n",
        "\n",
        "            # Get predicted label and probability for the current image\n",
        "\n",
        "\n",
        "            # Append the results for this image to the CSV file\n",
        "            writer.writerow({\n",
        "                \"image_name\": img_name,\n",
        "                \"actual_label\": label.item(),\n",
        "                \"predicted_label\": predicted_label,\n",
        "                \"prediction_probability\": prediction_probability,\n",
        "                \"top_hateful_word_1\": top_hateful_words[0][0],\n",
        "                \"confidence_1\": top_hateful_words[0][1],\n",
        "                \"top_hateful_word_2\": top_hateful_words[1][0] if len(top_hateful_words) > 1 else \"\",\n",
        "                \"confidence_2\": top_hateful_words[1][1] if len(top_hateful_words) > 1 else 0,\n",
        "                \"top_hateful_word_3\": top_hateful_words[2][0] if len(top_hateful_words) > 2 else \"\",\n",
        "                \"confidence_3\": top_hateful_words[2][1] if len(top_hateful_words) > 2 else 0,\n",
        "                \"top_hateful_word_4\": top_hateful_words[3][0] if len(top_hateful_words) > 3 else \"\",\n",
        "                \"confidence_4\": top_hateful_words[3][1] if len(top_hateful_words) > 3 else 0,\n",
        "                \"top_hateful_word_5\": top_hateful_words[4][0] if len(top_hateful_words) > 4 else \"\",\n",
        "                \"confidence_5\": top_hateful_words[4][1] if len(top_hateful_words) > 4 else 0,\n",
        "                \"top_hateful_word_6\": top_hateful_words[5][0] if len(top_hateful_words) > 5 else \"\",\n",
        "                \"confidence_6\": top_hateful_words[5][1] if len(top_hateful_words) > 5 else 0,\n",
        "                \"top_hateful_word_7\": top_hateful_words[6][0] if len(top_hateful_words) > 6 else \"\",\n",
        "                \"confidence_7\": top_hateful_words[6][1] if len(top_hateful_words) > 6 else 0,\n",
        "                \"top_hateful_word_8\": top_hateful_words[7][0] if len(top_hateful_words) > 7 else \"\",\n",
        "                \"confidence_8\": top_hateful_words[7][1] if len(top_hateful_words) > 7 else 0,\n",
        "            })\n",
        "\n",
        "    loss = self.cross_entropy_loss(logits, lab)\n",
        "    self.log('train_loss', loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "     # print(self)\n",
        "      lab,txt,img,name,test_description= val_batch\n",
        "      #print(lab)\n",
        "      lab = val_batch[lab]\n",
        "      #print(lab)\n",
        "      txt = val_batch[txt]\n",
        "      img = val_batch[img]\n",
        "      test_description=val_batch[test_description]\n",
        "\n",
        "\n",
        "      logits= self.forward1(txt,img)\n",
        "      probabilities = torch.softmax(logits, dim=1)\n",
        "      prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "      predicted_label = predicted_label.detach().cpu().numpy()\n",
        "      prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "      tmp = predicted_label\n",
        "      #print(np.argmax(logits.detach().cpu().numpy(),axis=-1))\n",
        "      loss = self.cross_entropy_loss(logits, lab)\n",
        "      #print(loss)\n",
        "      lab = lab.detach().cpu().numpy()\n",
        "      for img_name, text_description, label,predicted_label,prediction_probability in zip(name, test_description, lab,predicted_label,prediction_probability):\n",
        "            predictions = []\n",
        "            word_impact = []\n",
        "            predicted_label=predicted_label.item()\n",
        "            prediction_probability=prediction_probability.item()\n",
        "            text_description_words = text_description.split()\n",
        "\n",
        "            # Iterate over each text description in the list\n",
        "            for description in text_description_words:\n",
        "                masked_text = ' '.join([word for word in text_description_words if word != description])\n",
        "                word_input = description\n",
        "                word_logits = self.forward(txt, img, masked_text)\n",
        "               # print(\"word_logits\",word_logits)\n",
        "                word_prediction = F.softmax(word_logits, dim=1)\n",
        "                #print(\"word_prediction\",word_prediction)\n",
        "                hate_confidence = word_prediction[0][1].item()\n",
        "                #print(\"hate_confidence\",hate_confidence)\n",
        "                word_impact.append((description, hate_confidence))\n",
        "\n",
        "            # Sort the words based on hate confidence in descending order\n",
        "            sorted_word_impact = sorted(word_impact, key=lambda x: x[1], )\n",
        "\n",
        "            # Take the top 3 most hateful words if available, otherwise fill with placeholders\n",
        "            top_hateful_words = sorted_word_impact[:8] if sorted_word_impact else [(\"\", 0)] * 8\n",
        "\n",
        "#*****************************************\n",
        "\n",
        "      self.log('val_acc', accuracy_score(lab,tmp))\n",
        "      #print('val_acc', accuracy_score(lab,tmp))\n",
        "      self.log('val_roc_auc',roc_auc_score(lab,tmp))\n",
        "      #print('val_roc_auc',roc_auc_score(lab,tmp))\n",
        "      self.log('val_loss', loss)\n",
        "      #print('val_loss', loss)\n",
        "      tqdm_dict = {'val_acc': accuracy_score(lab,tmp)}\n",
        "      #print('val_acc', accuracy_score(lab,tmp))\n",
        "      return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "      'val_f1 offensive': f1_score(lab,tmp,average='macro')\n",
        "      }\n",
        "\n",
        "  def validation_epoch_end(self, validation_step_outputs):\n",
        "    #print(validation_step_outputs)\n",
        "    outs = []\n",
        "    outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14,outs16,outs17 = \\\n",
        "    [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "    for out in validation_step_outputs:\n",
        "      #print(out)\n",
        "      outs.append(out['progress_bar']['val_acc'])\n",
        "      outs14.append(out['val_f1 offensive'])\n",
        "\n",
        "    self.log('val_acc_all_offn', sum(outs)/len(outs))\n",
        "    self.log('val_f1 offensive', sum(outs14)/len(outs14))\n",
        "    print(f'***val_acc_all_offn at epoch end {sum(outs)/len(outs)}****')\n",
        "    print(f'***val_f1 offensive at epoch end {sum(outs14)/len(outs14)}****')\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "        lab,txt,img,name,text_description= batch\n",
        "\n",
        "        lab = batch[lab]\n",
        "        name=batch[name]\n",
        "        test_description=batch[text_description]\n",
        "        txt = batch[txt]\n",
        "        img = batch[img]\n",
        "        logits= self.forward1(txt,img)\n",
        "\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        prediction_probability, predicted_label = torch.max(probabilities, dim=1)\n",
        "        predicted_label = predicted_label.detach().cpu().numpy()\n",
        "        tmp=predicted_label\n",
        "        prediction_probability = prediction_probability.detach().cpu().numpy()\n",
        "\n",
        "        csv_file_path = f\"hateful_words_test_results_epoch__{self.current_epoch + 1}.csv\"\n",
        "        with open(csv_file_path, mode='a', newline='') as file:\n",
        "            fieldnames = [\"image_name\", \"actual_label\", \"predicted_label\", \"prediction_probability\",\n",
        "                          \"top_hateful_word_1\", \"confidence_1\", \"top_hateful_word_2\", \"confidence_2\",\n",
        "                          \"top_hateful_word_3\", \"confidence_3\",\n",
        "                          \"top_hateful_word_4\", \"confidence_4\",\n",
        "                          \"top_hateful_word_5\", \"confidence_5\",\n",
        "                          \"top_hateful_word_6\", \"confidence_6\",\n",
        "                          \"top_hateful_word_7\", \"confidence_7\",\n",
        "                          \"top_hateful_word_8\", \"confidence_8\"\n",
        "                         ]\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "            # Write header only if the file is empty\n",
        "            if file.tell() == 0:\n",
        "                writer.writeheader()\n",
        "\n",
        "            for img_name, text_description, label,predicted_label,prediction_probability in zip(name, test_description, lab,predicted_label,prediction_probability):\n",
        "                predictions = []\n",
        "                word_impact = []\n",
        "                predicted_label=predicted_label.item()\n",
        "                prediction_probability=prediction_probability.item()\n",
        "                text_description_words = text_description.split()\n",
        "\n",
        "                # Iterate over each text description in the list\n",
        "                for description in text_description_words:\n",
        "                    masked_text = ' '.join([word for word in text_description_words if word != description])\n",
        "                    word_input = description\n",
        "                    word_logits = self.forward(txt, img, masked_text)\n",
        "                   # print(\"word_logits\",word_logits)\n",
        "                    word_prediction = F.softmax(word_logits, dim=1)\n",
        "                    #print(\"word_prediction\",word_prediction)\n",
        "                    hate_confidence = word_prediction[0][1].item()\n",
        "                    #print(\"hate_confidence\",hate_confidence)\n",
        "                    word_impact.append((description, hate_confidence))\n",
        "\n",
        "                # Sort the words based on hate confidence in descending order\n",
        "                sorted_word_impact = sorted(word_impact, key=lambda x: x[1], )\n",
        "\n",
        "                # Take the top 3 most hateful words if available, otherwise fill with placeholders\n",
        "                top_hateful_words = sorted_word_impact[:8] if sorted_word_impact else [(\"\", 0)] * 8\n",
        "\n",
        "                # Get predicted label and probability for the current image\n",
        "\n",
        "\n",
        "                # Append the results for this image to the CSV file\n",
        "                writer.writerow({\n",
        "                    \"image_name\": img_name,\n",
        "                    \"actual_label\": label.item(),\n",
        "                    \"predicted_label\": predicted_label,\n",
        "                    \"prediction_probability\": prediction_probability,\n",
        "                    \"top_hateful_word_1\": top_hateful_words[0][0],\n",
        "                    \"confidence_1\": top_hateful_words[0][1],\n",
        "                    \"top_hateful_word_2\": top_hateful_words[1][0] if len(top_hateful_words) > 1 else \"\",\n",
        "                    \"confidence_2\": top_hateful_words[1][1] if len(top_hateful_words) > 1 else 0,\n",
        "                    \"top_hateful_word_3\": top_hateful_words[2][0] if len(top_hateful_words) > 2 else \"\",\n",
        "                    \"confidence_3\": top_hateful_words[2][1] if len(top_hateful_words) > 2 else 0,\n",
        "                    \"top_hateful_word_4\": top_hateful_words[3][0] if len(top_hateful_words) > 3 else \"\",\n",
        "                    \"confidence_4\": top_hateful_words[3][1] if len(top_hateful_words) > 3 else 0,\n",
        "                    \"top_hateful_word_5\": top_hateful_words[4][0] if len(top_hateful_words) > 4 else \"\",\n",
        "                    \"confidence_5\": top_hateful_words[4][1] if len(top_hateful_words) > 4 else 0,\n",
        "                    \"top_hateful_word_6\": top_hateful_words[5][0] if len(top_hateful_words) > 5 else \"\",\n",
        "                    \"confidence_6\": top_hateful_words[5][1] if len(top_hateful_words) > 5 else 0,\n",
        "                    \"top_hateful_word_7\": top_hateful_words[6][0] if len(top_hateful_words) > 6 else \"\",\n",
        "                    \"confidence_7\": top_hateful_words[6][1] if len(top_hateful_words) > 6 else 0,\n",
        "                    \"top_hateful_word_8\": top_hateful_words[7][0] if len(top_hateful_words) > 7 else \"\",\n",
        "                    \"confidence_8\": top_hateful_words[7][1] if len(top_hateful_words) > 7 else 0,\n",
        "                })\n",
        "        #****************************************************************\n",
        "        loss = self.cross_entropy_loss(logits, lab)\n",
        "        lab = lab.detach().cpu().numpy()\n",
        "        self.log('test_acc', accuracy_score(lab,tmp))\n",
        "        self.log('test_roc_auc',roc_auc_score(lab,tmp))\n",
        "        self.log('test_loss', loss)\n",
        "        tqdm_dict = {'test_acc': accuracy_score(lab,tmp)}\n",
        "        #labels = [\"hate\" if pred == 1 else \"not hate\" for pred in tmp]\n",
        "\n",
        "        return {\n",
        "                'progress_bar': tqdm_dict,\n",
        "                'test_acc': accuracy_score(lab,tmp),\n",
        "                'test_f1_score': f1_score(lab,tmp,average='macro'),\n",
        "                                  #predicted label\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "      # OPTIONAL\n",
        "      outs = []\n",
        "      outs1,outs2,outs3,outs4,outs5,outs6,outs7,outs8,outs9,outs10,outs11,outs12,outs13,outs14 = \\\n",
        "      [],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
        "      for out in outputs:\n",
        "        outs.append(out['test_acc'])\n",
        "        outs2.append(out['test_f1_score'])\n",
        "      self.log('test_acc', sum(outs)/len(outs))\n",
        "      self.log('test_f1_score', sum(outs2)/len(outs2))\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    # optimizer = torch.optim.Adam(self.parameters(), lr=3e-2)\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
        "    #print(torch.optim.Adam(self.parameters(), lr=1e-5))\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "class HmDataModule(pl.LightningDataModule):\n",
        "\n",
        "\n",
        "  def setup(self, stage):\n",
        "\n",
        "    self.hm_train = t_p\n",
        "\n",
        "    self.hm_val = v_p\n",
        "\n",
        "    # self.hm_test = test\n",
        "    self.hm_test = te_p\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.hm_train, batch_size=128, drop_last=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.hm_val, batch_size=128, drop_last=True)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.hm_test, batch_size=128, drop_last=True)\n",
        "data_module = HmDataModule()\n",
        "#print(data_module)\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "     monitor='val_acc_all_offn',\n",
        "     dirpath='Jitendra/',\n",
        "     filename= \"epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}\",#'epoch{epoch:02d}-val_f1_all_offn{val_acc_all_offn:.2f}',\n",
        "     auto_insert_metric_name=False,\n",
        "     save_top_k=1,\n",
        "    mode=\"max\",\n",
        " )\n",
        "all_callbacks = []\n",
        "all_callbacks.append(checkpoint_callback)\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(42, workers=True)\n",
        "hm_model = Classifier()\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "trainer = pl.Trainer(gpus=gpus,deterministic=True,max_epochs=1,precision=16,callbacks=all_callbacks)\n",
        "trainer.fit(hm_model, data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68075f9",
      "metadata": {
        "id": "e68075f9",
        "outputId": "9453ad73-9c4b-4650-9fe5-111a6235e1de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Restoring states from the checkpoint path at /DATA/atul_2221cs20/jitendra/Jitendra/epoch00-val_f1_all_offn0.79.ckpt\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Classifier:\n\tMissing key(s) in state_dict: \"clip_model.positional_embedding\", \"clip_model.text_projection\", \"clip_model.logit_scale\", \"clip_model.visual.class_embedding\", \"clip_model.visual.positional_embedding\", \"clip_model.visual.proj\", \"clip_model.visual.conv1.weight\", \"clip_model.visual.ln_pre.weight\", \"clip_model.visual.ln_pre.bias\", \"clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.0.ln_1.weight\", \"clip_model.visual.transformer.resblocks.0.ln_1.bias\", \"clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.0.ln_2.weight\", \"clip_model.visual.transformer.resblocks.0.ln_2.bias\", \"clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.1.ln_1.weight\", \"clip_model.visual.transformer.resblocks.1.ln_1.bias\", \"clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.1.ln_2.weight\", \"clip_model.visual.transformer.resblocks.1.ln_2.bias\", \"clip_model.visual.transformer.resblocks.2.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.2.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.2.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.2.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.2.ln_1.weight\", \"clip_model.visual.transformer.resblocks.2.ln_1.bias\", \"clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.2.ln_2.weight\", \"clip_model.visual.transformer.resblocks.2.ln_2.bias\", \"clip_model.visual.transformer.resblocks.3.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.3.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.3.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.3.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.3.ln_1.weight\", \"clip_model.visual.transformer.resblocks.3.ln_1.bias\", \"clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.3.ln_2.weight\", \"clip_model.visual.transformer.resblocks.3.ln_2.bias\", \"clip_model.visual.transformer.resblocks.4.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.4.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.4.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.4.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.4.ln_1.weight\", \"clip_model.visual.transformer.resblocks.4.ln_1.bias\", \"clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.4.ln_2.weight\", \"clip_model.visual.transformer.resblocks.4.ln_2.bias\", \"clip_model.visual.transformer.resblocks.5.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.5.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.5.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.5.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.5.ln_1.weight\", \"clip_model.visual.transformer.resblocks.5.ln_1.bias\", \"clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.5.ln_2.weight\", \"clip_model.visual.transformer.resblocks.5.ln_2.bias\", \"clip_model.visual.transformer.resblocks.6.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.6.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.6.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.6.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.6.ln_1.weight\", \"clip_model.visual.transformer.resblocks.6.ln_1.bias\", \"clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.6.ln_2.weight\", \"clip_model.visual.transformer.resblocks.6.ln_2.bias\", \"clip_model.visual.transformer.resblocks.7.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.7.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.7.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.7.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.7.ln_1.weight\", \"clip_model.visual.transformer.resblocks.7.ln_1.bias\", \"clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.7.ln_2.weight\", \"clip_model.visual.transformer.resblocks.7.ln_2.bias\", \"clip_model.visual.transformer.resblocks.8.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.8.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.8.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.8.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.8.ln_1.weight\", \"clip_model.visual.transformer.resblocks.8.ln_1.bias\", \"clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.8.ln_2.weight\", \"clip_model.visual.transformer.resblocks.8.ln_2.bias\", \"clip_model.visual.transformer.resblocks.9.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.9.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.9.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.9.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.9.ln_1.weight\", \"clip_model.visual.transformer.resblocks.9.ln_1.bias\", \"clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.9.ln_2.weight\", \"clip_model.visual.transformer.resblocks.9.ln_2.bias\", \"clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.10.ln_1.weight\", \"clip_model.visual.transformer.resblocks.10.ln_1.bias\", \"clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.10.ln_2.weight\", \"clip_model.visual.transformer.resblocks.10.ln_2.bias\", \"clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.11.ln_1.weight\", \"clip_model.visual.transformer.resblocks.11.ln_1.bias\", \"clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.11.ln_2.weight\", \"clip_model.visual.transformer.resblocks.11.ln_2.bias\", \"clip_model.visual.ln_post.weight\", \"clip_model.visual.ln_post.bias\", \"clip_model.transformer.resblocks.0.attn.in_proj_weight\", \"clip_model.transformer.resblocks.0.attn.in_proj_bias\", \"clip_model.transformer.resblocks.0.attn.out_proj.weight\", \"clip_model.transformer.resblocks.0.attn.out_proj.bias\", \"clip_model.transformer.resblocks.0.ln_1.weight\", \"clip_model.transformer.resblocks.0.ln_1.bias\", \"clip_model.transformer.resblocks.0.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.0.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.0.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.0.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.0.ln_2.weight\", \"clip_model.transformer.resblocks.0.ln_2.bias\", \"clip_model.transformer.resblocks.1.attn.in_proj_weight\", \"clip_model.transformer.resblocks.1.attn.in_proj_bias\", \"clip_model.transformer.resblocks.1.attn.out_proj.weight\", \"clip_model.transformer.resblocks.1.attn.out_proj.bias\", \"clip_model.transformer.resblocks.1.ln_1.weight\", \"clip_model.transformer.resblocks.1.ln_1.bias\", \"clip_model.transformer.resblocks.1.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.1.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.1.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.1.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.1.ln_2.weight\", \"clip_model.transformer.resblocks.1.ln_2.bias\", \"clip_model.transformer.resblocks.2.attn.in_proj_weight\", \"clip_model.transformer.resblocks.2.attn.in_proj_bias\", \"clip_model.transformer.resblocks.2.attn.out_proj.weight\", \"clip_model.transformer.resblocks.2.attn.out_proj.bias\", \"clip_model.transformer.resblocks.2.ln_1.weight\", \"clip_model.transformer.resblocks.2.ln_1.bias\", \"clip_model.transformer.resblocks.2.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.2.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.2.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.2.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.2.ln_2.weight\", \"clip_model.transformer.resblocks.2.ln_2.bias\", \"clip_model.transformer.resblocks.3.attn.in_proj_weight\", \"clip_model.transformer.resblocks.3.attn.in_proj_bias\", \"clip_model.transformer.resblocks.3.attn.out_proj.weight\", \"clip_model.transformer.resblocks.3.attn.out_proj.bias\", \"clip_model.transformer.resblocks.3.ln_1.weight\", \"clip_model.transformer.resblocks.3.ln_1.bias\", \"clip_model.transformer.resblocks.3.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.3.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.3.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.3.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.3.ln_2.weight\", \"clip_model.transformer.resblocks.3.ln_2.bias\", \"clip_model.transformer.resblocks.4.attn.in_proj_weight\", \"clip_model.transformer.resblocks.4.attn.in_proj_bias\", \"clip_model.transformer.resblocks.4.attn.out_proj.weight\", \"clip_model.transformer.resblocks.4.attn.out_proj.bias\", \"clip_model.transformer.resblocks.4.ln_1.weight\", \"clip_model.transformer.resblocks.4.ln_1.bias\", \"clip_model.transformer.resblocks.4.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.4.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.4.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.4.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.4.ln_2.weight\", \"clip_model.transformer.resblocks.4.ln_2.bias\", \"clip_model.transformer.resblocks.5.attn.in_proj_weight\", \"clip_model.transformer.resblocks.5.attn.in_proj_bias\", \"clip_model.transformer.resblocks.5.attn.out_proj.weight\", \"clip_model.transformer.resblocks.5.attn.out_proj.bias\", \"clip_model.transformer.resblocks.5.ln_1.weight\", \"clip_model.transformer.resblocks.5.ln_1.bias\", \"clip_model.transformer.resblocks.5.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.5.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.5.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.5.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.5.ln_2.weight\", \"clip_model.transformer.resblocks.5.ln_2.bias\", \"clip_model.transformer.resblocks.6.attn.in_proj_weight\", \"clip_model.transformer.resblocks.6.attn.in_proj_bias\", \"clip_model.transformer.resblocks.6.attn.out_proj.weight\", \"clip_model.transformer.resblocks.6.attn.out_proj.bias\", \"clip_model.transformer.resblocks.6.ln_1.weight\", \"clip_model.transformer.resblocks.6.ln_1.bias\", \"clip_model.transformer.resblocks.6.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.6.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.6.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.6.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.6.ln_2.weight\", \"clip_model.transformer.resblocks.6.ln_2.bias\", \"clip_model.transformer.resblocks.7.attn.in_proj_weight\", \"clip_model.transformer.resblocks.7.attn.in_proj_bias\", \"clip_model.transformer.resblocks.7.attn.out_proj.weight\", \"clip_model.transformer.resblocks.7.attn.out_proj.bias\", \"clip_model.transformer.resblocks.7.ln_1.weight\", \"clip_model.transformer.resblocks.7.ln_1.bias\", \"clip_model.transformer.resblocks.7.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.7.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.7.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.7.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.7.ln_2.weight\", \"clip_model.transformer.resblocks.7.ln_2.bias\", \"clip_model.transformer.resblocks.8.attn.in_proj_weight\", \"clip_model.transformer.resblocks.8.attn.in_proj_bias\", \"clip_model.transformer.resblocks.8.attn.out_proj.weight\", \"clip_model.transformer.resblocks.8.attn.out_proj.bias\", \"clip_model.transformer.resblocks.8.ln_1.weight\", \"clip_model.transformer.resblocks.8.ln_1.bias\", \"clip_model.transformer.resblocks.8.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.8.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.8.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.8.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.8.ln_2.weight\", \"clip_model.transformer.resblocks.8.ln_2.bias\", \"clip_model.transformer.resblocks.9.attn.in_proj_weight\", \"clip_model.transformer.resblocks.9.attn.in_proj_bias\", \"clip_model.transformer.resblocks.9.attn.out_proj.weight\", \"clip_model.transformer.resblocks.9.attn.out_proj.bias\", \"clip_model.transformer.resblocks.9.ln_1.weight\", \"clip_model.transformer.resblocks.9.ln_1.bias\", \"clip_model.transformer.resblocks.9.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.9.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.9.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.9.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.9.ln_2.weight\", \"clip_model.transformer.resblocks.9.ln_2.bias\", \"clip_model.transformer.resblocks.10.attn.in_proj_weight\", \"clip_model.transformer.resblocks.10.attn.in_proj_bias\", \"clip_model.transformer.resblocks.10.attn.out_proj.weight\", \"clip_model.transformer.resblocks.10.attn.out_proj.bias\", \"clip_model.transformer.resblocks.10.ln_1.weight\", \"clip_model.transformer.resblocks.10.ln_1.bias\", \"clip_model.transformer.resblocks.10.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.10.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.10.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.10.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.10.ln_2.weight\", \"clip_model.transformer.resblocks.10.ln_2.bias\", \"clip_model.transformer.resblocks.11.attn.in_proj_weight\", \"clip_model.transformer.resblocks.11.attn.in_proj_bias\", \"clip_model.transformer.resblocks.11.attn.out_proj.weight\", \"clip_model.transformer.resblocks.11.attn.out_proj.bias\", \"clip_model.transformer.resblocks.11.ln_1.weight\", \"clip_model.transformer.resblocks.11.ln_1.bias\", \"clip_model.transformer.resblocks.11.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.11.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.11.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.11.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.11.ln_2.weight\", \"clip_model.transformer.resblocks.11.ln_2.bias\", \"clip_model.token_embedding.weight\", \"clip_model.ln_final.weight\", \"clip_model.ln_final.bias\", \"image_classifier.clip_model.positional_embedding\", \"image_classifier.clip_model.text_projection\", \"image_classifier.clip_model.logit_scale\", \"image_classifier.clip_model.visual.class_embedding\", \"image_classifier.clip_model.visual.positional_embedding\", \"image_classifier.clip_model.visual.proj\", \"image_classifier.clip_model.visual.conv1.weight\", \"image_classifier.clip_model.visual.ln_pre.weight\", \"image_classifier.clip_model.visual.ln_pre.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_2.bias\", \"image_classifier.clip_model.visual.ln_post.weight\", \"image_classifier.clip_model.visual.ln_post.bias\", \"image_classifier.clip_model.transformer.resblocks.0.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.0.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.0.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.0.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.0.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.0.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.0.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.0.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.1.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.1.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.1.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.1.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.1.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.1.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.1.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.1.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.2.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.2.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.2.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.2.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.2.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.2.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.2.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.2.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.3.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.3.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.3.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.3.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.3.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.3.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.3.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.3.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.4.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.4.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.4.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.4.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.4.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.4.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.4.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.4.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.5.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.5.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.5.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.5.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.5.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.5.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.5.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.5.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.6.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.6.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.6.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.6.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.6.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.6.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.6.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.6.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.7.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.7.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.7.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.7.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.7.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.7.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.7.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.7.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.8.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.8.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.8.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.8.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.8.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.8.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.8.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.8.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.9.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.9.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.9.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.9.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.9.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.9.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.9.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.9.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.10.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.10.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.10.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.10.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.10.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.10.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.10.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.10.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.11.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.11.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.11.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.11.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.11.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.11.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.11.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.11.ln_2.bias\", \"image_classifier.clip_model.token_embedding.weight\", \"image_classifier.clip_model.ln_final.weight\", \"image_classifier.clip_model.ln_final.bias\", \"image_classifier.attention_layer.query_proj.weight\", \"image_classifier.attention_layer.query_proj.bias\", \"image_classifier.attention_layer.key_proj.weight\", \"image_classifier.attention_layer.key_proj.bias\", \"image_classifier.attention_layer.value_proj.weight\", \"image_classifier.attention_layer.value_proj.bias\", \"image_classifier.classifier.fc1.weight\", \"image_classifier.classifier.fc1.bias\", \"image_classifier.classifier.text_embedding.weight\", \"image_classifier.classifier.text_embedding.bias\", \"image_classifier.classifier.output_layer.weight\", \"image_classifier.classifier.output_layer.bias\". \n\tsize mismatch for MFB.proj_q.weight: copying a param with shape torch.Size([16384, 768]) from checkpoint, the shape in current model is torch.Size([16384, 512]).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m~/jitendra/ipykernel_838465/2158617681.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/DATA/atul_2221cs20/jitendra/Jitendra/epoch00-val_f1_all_offn0.79.ckpt'\u001b[0m \u001b[0;31m# put ckpt_path according to the path output in the previous cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#ckpt_path = \"/DATA/atul_2221cs20/jitendra/Jitendra/Current.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         return call._call_and_handle_interrupt(\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_modules_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: configuring sharded model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_quantization_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36mrestore_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# restore model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# reset metrics states on non-rank 0 as all states have been accumulated on rank 0 via syncing on checkpointing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mload_model_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_optimizer_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/jitu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Classifier:\n\tMissing key(s) in state_dict: \"clip_model.positional_embedding\", \"clip_model.text_projection\", \"clip_model.logit_scale\", \"clip_model.visual.class_embedding\", \"clip_model.visual.positional_embedding\", \"clip_model.visual.proj\", \"clip_model.visual.conv1.weight\", \"clip_model.visual.ln_pre.weight\", \"clip_model.visual.ln_pre.bias\", \"clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.0.ln_1.weight\", \"clip_model.visual.transformer.resblocks.0.ln_1.bias\", \"clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.0.ln_2.weight\", \"clip_model.visual.transformer.resblocks.0.ln_2.bias\", \"clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.1.ln_1.weight\", \"clip_model.visual.transformer.resblocks.1.ln_1.bias\", \"clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.1.ln_2.weight\", \"clip_model.visual.transformer.resblocks.1.ln_2.bias\", \"clip_model.visual.transformer.resblocks.2.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.2.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.2.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.2.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.2.ln_1.weight\", \"clip_model.visual.transformer.resblocks.2.ln_1.bias\", \"clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.2.ln_2.weight\", \"clip_model.visual.transformer.resblocks.2.ln_2.bias\", \"clip_model.visual.transformer.resblocks.3.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.3.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.3.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.3.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.3.ln_1.weight\", \"clip_model.visual.transformer.resblocks.3.ln_1.bias\", \"clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.3.ln_2.weight\", \"clip_model.visual.transformer.resblocks.3.ln_2.bias\", \"clip_model.visual.transformer.resblocks.4.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.4.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.4.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.4.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.4.ln_1.weight\", \"clip_model.visual.transformer.resblocks.4.ln_1.bias\", \"clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.4.ln_2.weight\", \"clip_model.visual.transformer.resblocks.4.ln_2.bias\", \"clip_model.visual.transformer.resblocks.5.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.5.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.5.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.5.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.5.ln_1.weight\", \"clip_model.visual.transformer.resblocks.5.ln_1.bias\", \"clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.5.ln_2.weight\", \"clip_model.visual.transformer.resblocks.5.ln_2.bias\", \"clip_model.visual.transformer.resblocks.6.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.6.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.6.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.6.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.6.ln_1.weight\", \"clip_model.visual.transformer.resblocks.6.ln_1.bias\", \"clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.6.ln_2.weight\", \"clip_model.visual.transformer.resblocks.6.ln_2.bias\", \"clip_model.visual.transformer.resblocks.7.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.7.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.7.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.7.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.7.ln_1.weight\", \"clip_model.visual.transformer.resblocks.7.ln_1.bias\", \"clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.7.ln_2.weight\", \"clip_model.visual.transformer.resblocks.7.ln_2.bias\", \"clip_model.visual.transformer.resblocks.8.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.8.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.8.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.8.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.8.ln_1.weight\", \"clip_model.visual.transformer.resblocks.8.ln_1.bias\", \"clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.8.ln_2.weight\", \"clip_model.visual.transformer.resblocks.8.ln_2.bias\", \"clip_model.visual.transformer.resblocks.9.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.9.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.9.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.9.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.9.ln_1.weight\", \"clip_model.visual.transformer.resblocks.9.ln_1.bias\", \"clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.9.ln_2.weight\", \"clip_model.visual.transformer.resblocks.9.ln_2.bias\", \"clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.10.ln_1.weight\", \"clip_model.visual.transformer.resblocks.10.ln_1.bias\", \"clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.10.ln_2.weight\", \"clip_model.visual.transformer.resblocks.10.ln_2.bias\", \"clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.11.ln_1.weight\", \"clip_model.visual.transformer.resblocks.11.ln_1.bias\", \"clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.11.ln_2.weight\", \"clip_model.visual.transformer.resblocks.11.ln_2.bias\", \"clip_model.visual.ln_post.weight\", \"clip_model.visual.ln_post.bias\", \"clip_model.transformer.resblocks.0.attn.in_proj_weight\", \"clip_model.transformer.resblocks.0.attn.in_proj_bias\", \"clip_model.transformer.resblocks.0.attn.out_proj.weight\", \"clip_model.transformer.resblocks.0.attn.out_proj.bias\", \"clip_model.transformer.resblocks.0.ln_1.weight\", \"clip_model.transformer.resblocks.0.ln_1.bias\", \"clip_model.transformer.resblocks.0.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.0.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.0.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.0.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.0.ln_2.weight\", \"clip_model.transformer.resblocks.0.ln_2.bias\", \"clip_model.transformer.resblocks.1.attn.in_proj_weight\", \"clip_model.transformer.resblocks.1.attn.in_proj_bias\", \"clip_model.transformer.resblocks.1.attn.out_proj.weight\", \"clip_model.transformer.resblocks.1.attn.out_proj.bias\", \"clip_model.transformer.resblocks.1.ln_1.weight\", \"clip_model.transformer.resblocks.1.ln_1.bias\", \"clip_model.transformer.resblocks.1.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.1.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.1.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.1.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.1.ln_2.weight\", \"clip_model.transformer.resblocks.1.ln_2.bias\", \"clip_model.transformer.resblocks.2.attn.in_proj_weight\", \"clip_model.transformer.resblocks.2.attn.in_proj_bias\", \"clip_model.transformer.resblocks.2.attn.out_proj.weight\", \"clip_model.transformer.resblocks.2.attn.out_proj.bias\", \"clip_model.transformer.resblocks.2.ln_1.weight\", \"clip_model.transformer.resblocks.2.ln_1.bias\", \"clip_model.transformer.resblocks.2.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.2.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.2.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.2.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.2.ln_2.weight\", \"clip_model.transformer.resblocks.2.ln_2.bias\", \"clip_model.transformer.resblocks.3.attn.in_proj_weight\", \"clip_model.transformer.resblocks.3.attn.in_proj_bias\", \"clip_model.transformer.resblocks.3.attn.out_proj.weight\", \"clip_model.transformer.resblocks.3.attn.out_proj.bias\", \"clip_model.transformer.resblocks.3.ln_1.weight\", \"clip_model.transformer.resblocks.3.ln_1.bias\", \"clip_model.transformer.resblocks.3.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.3.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.3.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.3.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.3.ln_2.weight\", \"clip_model.transformer.resblocks.3.ln_2.bias\", \"clip_model.transformer.resblocks.4.attn.in_proj_weight\", \"clip_model.transformer.resblocks.4.attn.in_proj_bias\", \"clip_model.transformer.resblocks.4.attn.out_proj.weight\", \"clip_model.transformer.resblocks.4.attn.out_proj.bias\", \"clip_model.transformer.resblocks.4.ln_1.weight\", \"clip_model.transformer.resblocks.4.ln_1.bias\", \"clip_model.transformer.resblocks.4.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.4.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.4.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.4.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.4.ln_2.weight\", \"clip_model.transformer.resblocks.4.ln_2.bias\", \"clip_model.transformer.resblocks.5.attn.in_proj_weight\", \"clip_model.transformer.resblocks.5.attn.in_proj_bias\", \"clip_model.transformer.resblocks.5.attn.out_proj.weight\", \"clip_model.transformer.resblocks.5.attn.out_proj.bias\", \"clip_model.transformer.resblocks.5.ln_1.weight\", \"clip_model.transformer.resblocks.5.ln_1.bias\", \"clip_model.transformer.resblocks.5.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.5.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.5.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.5.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.5.ln_2.weight\", \"clip_model.transformer.resblocks.5.ln_2.bias\", \"clip_model.transformer.resblocks.6.attn.in_proj_weight\", \"clip_model.transformer.resblocks.6.attn.in_proj_bias\", \"clip_model.transformer.resblocks.6.attn.out_proj.weight\", \"clip_model.transformer.resblocks.6.attn.out_proj.bias\", \"clip_model.transformer.resblocks.6.ln_1.weight\", \"clip_model.transformer.resblocks.6.ln_1.bias\", \"clip_model.transformer.resblocks.6.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.6.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.6.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.6.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.6.ln_2.weight\", \"clip_model.transformer.resblocks.6.ln_2.bias\", \"clip_model.transformer.resblocks.7.attn.in_proj_weight\", \"clip_model.transformer.resblocks.7.attn.in_proj_bias\", \"clip_model.transformer.resblocks.7.attn.out_proj.weight\", \"clip_model.transformer.resblocks.7.attn.out_proj.bias\", \"clip_model.transformer.resblocks.7.ln_1.weight\", \"clip_model.transformer.resblocks.7.ln_1.bias\", \"clip_model.transformer.resblocks.7.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.7.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.7.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.7.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.7.ln_2.weight\", \"clip_model.transformer.resblocks.7.ln_2.bias\", \"clip_model.transformer.resblocks.8.attn.in_proj_weight\", \"clip_model.transformer.resblocks.8.attn.in_proj_bias\", \"clip_model.transformer.resblocks.8.attn.out_proj.weight\", \"clip_model.transformer.resblocks.8.attn.out_proj.bias\", \"clip_model.transformer.resblocks.8.ln_1.weight\", \"clip_model.transformer.resblocks.8.ln_1.bias\", \"clip_model.transformer.resblocks.8.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.8.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.8.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.8.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.8.ln_2.weight\", \"clip_model.transformer.resblocks.8.ln_2.bias\", \"clip_model.transformer.resblocks.9.attn.in_proj_weight\", \"clip_model.transformer.resblocks.9.attn.in_proj_bias\", \"clip_model.transformer.resblocks.9.attn.out_proj.weight\", \"clip_model.transformer.resblocks.9.attn.out_proj.bias\", \"clip_model.transformer.resblocks.9.ln_1.weight\", \"clip_model.transformer.resblocks.9.ln_1.bias\", \"clip_model.transformer.resblocks.9.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.9.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.9.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.9.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.9.ln_2.weight\", \"clip_model.transformer.resblocks.9.ln_2.bias\", \"clip_model.transformer.resblocks.10.attn.in_proj_weight\", \"clip_model.transformer.resblocks.10.attn.in_proj_bias\", \"clip_model.transformer.resblocks.10.attn.out_proj.weight\", \"clip_model.transformer.resblocks.10.attn.out_proj.bias\", \"clip_model.transformer.resblocks.10.ln_1.weight\", \"clip_model.transformer.resblocks.10.ln_1.bias\", \"clip_model.transformer.resblocks.10.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.10.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.10.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.10.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.10.ln_2.weight\", \"clip_model.transformer.resblocks.10.ln_2.bias\", \"clip_model.transformer.resblocks.11.attn.in_proj_weight\", \"clip_model.transformer.resblocks.11.attn.in_proj_bias\", \"clip_model.transformer.resblocks.11.attn.out_proj.weight\", \"clip_model.transformer.resblocks.11.attn.out_proj.bias\", \"clip_model.transformer.resblocks.11.ln_1.weight\", \"clip_model.transformer.resblocks.11.ln_1.bias\", \"clip_model.transformer.resblocks.11.mlp.c_fc.weight\", \"clip_model.transformer.resblocks.11.mlp.c_fc.bias\", \"clip_model.transformer.resblocks.11.mlp.c_proj.weight\", \"clip_model.transformer.resblocks.11.mlp.c_proj.bias\", \"clip_model.transformer.resblocks.11.ln_2.weight\", \"clip_model.transformer.resblocks.11.ln_2.bias\", \"clip_model.token_embedding.weight\", \"clip_model.ln_final.weight\", \"clip_model.ln_final.bias\", \"image_classifier.clip_model.positional_embedding\", \"image_classifier.clip_model.text_projection\", \"image_classifier.clip_model.logit_scale\", \"image_classifier.clip_model.visual.class_embedding\", \"image_classifier.clip_model.visual.positional_embedding\", \"image_classifier.clip_model.visual.proj\", \"image_classifier.clip_model.visual.conv1.weight\", \"image_classifier.clip_model.visual.ln_pre.weight\", \"image_classifier.clip_model.visual.ln_pre.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.0.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.1.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.2.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.3.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.4.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.5.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.6.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.7.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.8.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.9.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.10.ln_2.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_1.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_1.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_2.weight\", \"image_classifier.clip_model.visual.transformer.resblocks.11.ln_2.bias\", \"image_classifier.clip_model.visual.ln_post.weight\", \"image_classifier.clip_model.visual.ln_post.bias\", \"image_classifier.clip_model.transformer.resblocks.0.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.0.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.0.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.0.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.0.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.0.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.0.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.0.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.0.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.1.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.1.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.1.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.1.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.1.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.1.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.1.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.1.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.1.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.2.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.2.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.2.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.2.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.2.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.2.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.2.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.2.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.2.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.3.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.3.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.3.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.3.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.3.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.3.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.3.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.3.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.3.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.4.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.4.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.4.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.4.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.4.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.4.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.4.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.4.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.4.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.5.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.5.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.5.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.5.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.5.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.5.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.5.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.5.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.5.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.6.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.6.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.6.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.6.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.6.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.6.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.6.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.6.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.6.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.7.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.7.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.7.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.7.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.7.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.7.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.7.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.7.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.7.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.8.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.8.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.8.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.8.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.8.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.8.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.8.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.8.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.8.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.9.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.9.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.9.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.9.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.9.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.9.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.9.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.9.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.9.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.10.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.10.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.10.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.10.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.10.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.10.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.10.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.10.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.10.ln_2.bias\", \"image_classifier.clip_model.transformer.resblocks.11.attn.in_proj_weight\", \"image_classifier.clip_model.transformer.resblocks.11.attn.in_proj_bias\", \"image_classifier.clip_model.transformer.resblocks.11.attn.out_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.11.attn.out_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.11.ln_1.weight\", \"image_classifier.clip_model.transformer.resblocks.11.ln_1.bias\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_fc.weight\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_fc.bias\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_proj.weight\", \"image_classifier.clip_model.transformer.resblocks.11.mlp.c_proj.bias\", \"image_classifier.clip_model.transformer.resblocks.11.ln_2.weight\", \"image_classifier.clip_model.transformer.resblocks.11.ln_2.bias\", \"image_classifier.clip_model.token_embedding.weight\", \"image_classifier.clip_model.ln_final.weight\", \"image_classifier.clip_model.ln_final.bias\", \"image_classifier.attention_layer.query_proj.weight\", \"image_classifier.attention_layer.query_proj.bias\", \"image_classifier.attention_layer.key_proj.weight\", \"image_classifier.attention_layer.key_proj.bias\", \"image_classifier.attention_layer.value_proj.weight\", \"image_classifier.attention_layer.value_proj.bias\", \"image_classifier.classifier.fc1.weight\", \"image_classifier.classifier.fc1.bias\", \"image_classifier.classifier.text_embedding.weight\", \"image_classifier.classifier.text_embedding.bias\", \"image_classifier.classifier.output_layer.weight\", \"image_classifier.classifier.output_layer.bias\". \n\tsize mismatch for MFB.proj_q.weight: copying a param with shape torch.Size([16384, 768]) from checkpoint, the shape in current model is torch.Size([16384, 512])."
          ]
        }
      ],
      "source": [
        "test_dataloader = DataLoader(dataset=te_p, batch_size=1478)\n",
        "\n",
        "ckpt_path = '/DATA/atul_2221cs20/jitendra/Jitendra/epoch00-val_f1_all_offn0.79.ckpt' # put ckpt_path according to the path output in the previous cell\n",
        "#ckpt_path = \"/DATA/atul_2221cs20/jitendra/Jitendra/Current.ckpt\"\n",
        "trainer.test(dataloaders=test_dataloader,ckpt_path=ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a9a24040",
      "metadata": {
        "id": "a9a24040"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2196a343",
      "metadata": {
        "id": "2196a343"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f76580bf",
      "metadata": {
        "id": "f76580bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7b26aad9",
      "metadata": {
        "id": "7b26aad9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8839a82",
      "metadata": {
        "id": "c8839a82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f293b357",
      "metadata": {
        "id": "f293b357"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518ecfc0",
      "metadata": {
        "id": "518ecfc0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727456d0",
      "metadata": {
        "id": "727456d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2278849",
      "metadata": {
        "id": "d2278849"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9470539",
      "metadata": {
        "id": "a9470539"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fdde453",
      "metadata": {
        "id": "2fdde453"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1936e318",
      "metadata": {
        "id": "1936e318"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}